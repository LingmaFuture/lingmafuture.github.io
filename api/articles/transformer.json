{"title":"Transformer 架构详解（含 PyTorch 代码）","uid":"48a8ab6165265f9c75721ce750feca94","slug":"transformer","date":"2025-08-12T16:00:00.000Z","updated":"2025-08-15T12:03:52.469Z","comments":true,"path":"api/articles/transformer.json","keywords":"blog, technology, programming","cover":"https://raw.githubusercontent.com/LingmaFuture/lingmafuture.github.io/refs/heads/main/images/6.png","content":"<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>读者对象：已具备基本深度学习与 PyTorch 基础，希望系统掌握 Transformer 各模块设计与实现的工程师&#x2F;学生。<br>文章目标：<strong>从实现角度深入讲清楚每个子模块</strong>（Scaled Dot-Product Attention、多头注意力、残差连接+LayerNorm、Position-wise FFN、位置编码、编码器&#x2F;解码器层与整模型），并给出<strong>可运行的最小化 PyTorch 参考实现</strong>与常见坑位排查清单。<br>题外话：背景演进（RNN&#x2F;Conv）不展开，直接上核心。</p></blockquote>\n<hr>\n<h2 id=\"总览（形状与数据流）\"><a href=\"#总览（形状与数据流）\" class=\"headerlink\" title=\"总览（形状与数据流）\"></a>总览（形状与数据流）</h2><p>我们统一使用 <strong>batch-first</strong> 约定：张量形状均为 <code>(B, S, D)</code>。  </p>\n<ul>\n<li><code>B</code>：batch size  </li>\n<li><code>S</code>：序列长度（<code>S_q</code>&#x2F;<code>S_k</code>&#x2F;<code>S_v</code> 在解码器交叉注意力中可能不同）  </li>\n<li><code>D</code>：模型隐藏维度 <code>d_model</code></li>\n</ul>\n<p>ASCII 拆解：</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[Token Embedding] + [Positional Encoding]</span><br><span class=\"line\">          │ (B,S,D)</span><br><span class=\"line\">          ▼</span><br><span class=\"line\">   ┌───────────── Encoder Layer × N ─────────────┐</span><br><span class=\"line\">   │  Self-Attn ── Add&amp;Norm ── FFN ── Add&amp;Norm   │</span><br><span class=\"line\">   └──────────────────────────────────────────────┘</span><br><span class=\"line\">          │ (B,S,D) = Encoder Memory (Keys/Values)</span><br><span class=\"line\">          ▼</span><br><span class=\"line\">   ┌───────────── Decoder Layer × N ─────────────┐</span><br><span class=\"line\">   │  Masked Self-Attn ─ Add&amp;Norm                │</span><br><span class=\"line\">   │  Cross Attn (Q=decoder, K,V=encoder)        │</span><br><span class=\"line\">   │               ─ Add&amp;Norm ─ FFN ─ Add&amp;Norm   │</span><br><span class=\"line\">   └──────────────────────────────────────────────┘</span><br><span class=\"line\">          │ (B,S,D)</span><br><span class=\"line\">          ▼</span><br><span class=\"line\">      [Linear → Softmax] → 生成下一个 token</span><br></pre></td></tr></table></figure>\n\n<hr>\n<h2 id=\"1-Scaled-Dot-Product-Attention（缩放点积注意力）\"><a href=\"#1-Scaled-Dot-Product-Attention（缩放点积注意力）\" class=\"headerlink\" title=\"1) Scaled Dot-Product Attention（缩放点积注意力）\"></a>1) Scaled Dot-Product Attention（缩放点积注意力）</h2><p><strong>公式</strong>：<br>[<br>\\mathrm{Attention}(Q,K,V)&#x3D;\\mathrm{softmax}!\\left(\\frac{QK^\\top}{\\sqrt{d_k}}+M\\right)V<br>]</p>\n<ul>\n<li><code>Q∈ℝ^&#123;B×h×S_q×d_k&#125;</code>, <code>K∈ℝ^&#123;B×h×S_k×d_k&#125;</code>, <code>V∈ℝ^&#123;B×h×S_k×d_v&#125;</code>  </li>\n<li><code>h</code> 为注意力头数；常取 <code>d_k=d_v=d_model/h</code>  </li>\n<li><code>M</code> 为掩码（mask），屏蔽无效或未来位置，形状可广播到 <code>(B,h,S_q,S_k)</code>；被屏蔽处取 <code>-inf</code></li>\n</ul>\n<p><strong>实现要点</strong>：</p>\n<ul>\n<li>除以 <code>sqrt(d_k)</code> 防止内积随维度增大导致 softmax 梯度极小；</li>\n<li>掩码一定要在 softmax 之前加到 logits 上；</li>\n<li><strong>全被屏蔽</strong>会导致 <code>softmax(-inf)=NaN</code>，推理&#x2F;训练前要确保至少一处可见。</li>\n</ul>\n<p><strong>参考实现</strong>：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch, math</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn.functional <span class=\"keyword\">as</span> F</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">scaled_dot_product_attention</span>(<span class=\"params\">Q, K, V, mask=<span class=\"literal\">None</span>, dropout_p=<span class=\"number\">0.0</span></span>):</span><br><span class=\"line\">    <span class=\"comment\"># Q,K,V: (B,h,S,dk/dv)</span></span><br><span class=\"line\">    scores = Q @ K.transpose(-<span class=\"number\">2</span>, -<span class=\"number\">1</span>) / math.sqrt(Q.size(-<span class=\"number\">1</span>))  <span class=\"comment\"># (B,h,S_q,S_k)</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> mask <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">        <span class=\"comment\"># 要求 mask 可广播至 scores 形状；True/1 表示「可见」更直观的话可改写</span></span><br><span class=\"line\">        scores = scores.masked_fill(mask == <span class=\"number\">0</span>, <span class=\"built_in\">float</span>(<span class=\"string\">&quot;-inf&quot;</span>))</span><br><span class=\"line\">    attn = F.softmax(scores, dim=-<span class=\"number\">1</span>)            <span class=\"comment\"># (B,h,S_q,S_k)</span></span><br><span class=\"line\">    attn = F.dropout(attn, p=dropout_p, training=Q.requires_grad)</span><br><span class=\"line\">    out = attn @ V                              <span class=\"comment\"># (B,h,S_q,dv)</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> out, attn</span><br></pre></td></tr></table></figure>\n\n<hr>\n<h2 id=\"2-Multi-Head-Attention（多头注意力）\"><a href=\"#2-Multi-Head-Attention（多头注意力）\" class=\"headerlink\" title=\"2) Multi-Head Attention（多头注意力）\"></a>2) Multi-Head Attention（多头注意力）</h2><p>核心思想：用多个线性投影将输入映射到不同子空间并行做注意力，然后拼接聚合，提高表达能力。</p>\n<p><strong>实现要点</strong>：</p>\n<ul>\n<li>线性层一次性生成 <code>Q,K,V</code>，再 <code>view</code>&#x2F;<code>reshape</code> 成 <code>(B,h,S,d_k)</code>；</li>\n<li>拼接时 <code>transpose+contiguous+view</code> 回到 <code>(B,S,D)</code>；</li>\n<li>统一用 <strong>batch-first</strong>，不与 PyTorch 自带 <code>nn.MultiheadAttention</code>（默认为 seq-first）混淆。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">MultiHeadAttention</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, d_model: <span class=\"built_in\">int</span>, num_heads: <span class=\"built_in\">int</span>, dropout: <span class=\"built_in\">float</span> = <span class=\"number\">0.0</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        <span class=\"keyword\">assert</span> d_model % num_heads == <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.d_model = d_model</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.h = num_heads</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.d_k = d_model // num_heads</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.qkv = nn.Linear(d_model, <span class=\"number\">3</span> * d_model, bias=<span class=\"literal\">False</span>)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.o_proj = nn.Linear(d_model, d_model, bias=<span class=\"literal\">False</span>)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.dropout = nn.Dropout(dropout)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x_q, x_kv, attn_mask=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">        <span class=\"comment\"># x_q: (B,S_q,D), x_kv: (B,S_k,D)</span></span><br><span class=\"line\">        B, Sq, _ = x_q.shape</span><br><span class=\"line\">        Sk = x_kv.shape[<span class=\"number\">1</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">        qkv_q = <span class=\"variable language_\">self</span>.qkv(x_q)                                  <span class=\"comment\"># (B,S_q,3D)</span></span><br><span class=\"line\">        qkv_kv = <span class=\"variable language_\">self</span>.qkv(x_kv)                                <span class=\"comment\"># 共享参数的简化实现</span></span><br><span class=\"line\">        Q = qkv_q[..., :<span class=\"variable language_\">self</span>.d_model]</span><br><span class=\"line\">        K = qkv_kv[..., <span class=\"variable language_\">self</span>.d_model:<span class=\"number\">2</span>*<span class=\"variable language_\">self</span>.d_model]</span><br><span class=\"line\">        V = qkv_kv[..., <span class=\"number\">2</span>*<span class=\"variable language_\">self</span>.d_model:]</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># (B,S,D) -&gt; (B,h,S,d_k)</span></span><br><span class=\"line\">        <span class=\"keyword\">def</span> <span class=\"title function_\">split_heads</span>(<span class=\"params\">t</span>):</span><br><span class=\"line\">            <span class=\"keyword\">return</span> t.view(B, -<span class=\"number\">1</span>, <span class=\"variable language_\">self</span>.h, <span class=\"variable language_\">self</span>.d_k).transpose(<span class=\"number\">1</span>, <span class=\"number\">2</span>).contiguous()</span><br><span class=\"line\">        Q, K, V = <span class=\"built_in\">map</span>(split_heads, (Q, K, V))</span><br><span class=\"line\"></span><br><span class=\"line\">        out, attn = scaled_dot_product_attention(Q, K, V, mask=attn_mask, dropout_p=<span class=\"variable language_\">self</span>.dropout.p)</span><br><span class=\"line\">        <span class=\"comment\"># (B,h,S_q,d_k) -&gt; (B,S_q,D)</span></span><br><span class=\"line\">        out = out.transpose(<span class=\"number\">1</span>, <span class=\"number\">2</span>).contiguous().view(B, Sq, <span class=\"variable language_\">self</span>.d_model)</span><br><span class=\"line\">        out = <span class=\"variable language_\">self</span>.o_proj(out)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> out, attn</span><br></pre></td></tr></table></figure>\n\n<p><strong>注意</strong>：实际工程中常将 <code>q_proj/k_proj/v_proj</code> 分开（尤其在解码器 cross-attn 中），此处为简洁性复用一套权重。</p>\n<hr>\n<h2 id=\"3-残差连接与-LayerNorm（Pre-LN-vs-Post-LN）\"><a href=\"#3-残差连接与-LayerNorm（Pre-LN-vs-Post-LN）\" class=\"headerlink\" title=\"3) 残差连接与 LayerNorm（Pre-LN vs Post-LN）\"></a>3) 残差连接与 LayerNorm（Pre-LN vs Post-LN）</h2><p>两种常见放置方式：</p>\n<ul>\n<li><strong>Post-LN（Vaswani17 原版）</strong>：<code>x = LN(x + Sublayer(x))</code></li>\n<li><strong>Pre-LN（更稳定的梯度传播，深层更常用）</strong>：<code>x = x + Sublayer(LN(x))</code></li>\n</ul>\n<p>本文实现采用 <strong>Pre-LN</strong>。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">ResidualBlock</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, d_model, sublayer, dropout=<span class=\"number\">0.0</span>, pre_norm=<span class=\"literal\">True</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.norm = nn.LayerNorm(d_model)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.sublayer = sublayer</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.dropout = nn.Dropout(dropout)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.pre_norm = pre_norm</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x, *args, **kwargs</span>):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"variable language_\">self</span>.pre_norm:</span><br><span class=\"line\">            y = <span class=\"variable language_\">self</span>.sublayer(<span class=\"variable language_\">self</span>.norm(x), *args, **kwargs)</span><br><span class=\"line\">            y = <span class=\"variable language_\">self</span>.dropout(y)</span><br><span class=\"line\">            <span class=\"keyword\">return</span> x + y</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            y = <span class=\"variable language_\">self</span>.sublayer(x, *args, **kwargs)</span><br><span class=\"line\">            y = <span class=\"variable language_\">self</span>.dropout(y)</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"variable language_\">self</span>.norm(x + y)</span><br></pre></td></tr></table></figure>\n\n<hr>\n<h2 id=\"4-Position-wise-FFN（逐位置前馈网络）\"><a href=\"#4-Position-wise-FFN（逐位置前馈网络）\" class=\"headerlink\" title=\"4) Position-wise FFN（逐位置前馈网络）\"></a>4) Position-wise FFN（逐位置前馈网络）</h2><p>论文为 <code>ReLU</code>，许多实现改用 <code>GELU</code>。中间维度 <code>d_ff</code> 通常取 <code>4×d_model</code>。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">PositionwiseFFN</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, d_model, d_ff, dropout=<span class=\"number\">0.0</span>, activation=<span class=\"string\">&quot;gelu&quot;</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.fc1 = nn.Linear(d_model, d_ff)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.fc2 = nn.Linear(d_ff, d_model)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.dropout = nn.Dropout(dropout)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.act = nn.GELU() <span class=\"keyword\">if</span> activation.lower()==<span class=\"string\">&quot;gelu&quot;</span> <span class=\"keyword\">else</span> nn.ReLU()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"variable language_\">self</span>.fc2(<span class=\"variable language_\">self</span>.dropout(<span class=\"variable language_\">self</span>.act(<span class=\"variable language_\">self</span>.fc1(x))))</span><br></pre></td></tr></table></figure>\n\n<hr>\n<h2 id=\"5-位置编码（Positional-Encoding）\"><a href=\"#5-位置编码（Positional-Encoding）\" class=\"headerlink\" title=\"5) 位置编码（Positional Encoding）\"></a>5) 位置编码（Positional Encoding）</h2><p>固定<strong>正弦&#x2F;余弦</strong>位置编码（可外推到更长序列）；也可用<strong>可学习</strong>位置嵌入。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> math</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">SinusoidalPositionalEncoding</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, d_model, max_len=<span class=\"number\">10000</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        pe = torch.zeros(max_len, d_model)</span><br><span class=\"line\">        pos = torch.arange(<span class=\"number\">0</span>, max_len, dtype=torch.<span class=\"built_in\">float</span>).unsqueeze(<span class=\"number\">1</span>)</span><br><span class=\"line\">        div = torch.exp(torch.arange(<span class=\"number\">0</span>, d_model, <span class=\"number\">2</span>, dtype=torch.<span class=\"built_in\">float</span>) *</span><br><span class=\"line\">                        -(math.log(<span class=\"number\">10000.0</span>) / d_model))</span><br><span class=\"line\">        pe[:, <span class=\"number\">0</span>::<span class=\"number\">2</span>] = torch.sin(pos * div)</span><br><span class=\"line\">        pe[:, <span class=\"number\">1</span>::<span class=\"number\">2</span>] = torch.cos(pos * div)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.register_buffer(<span class=\"string\">&quot;pe&quot;</span>, pe)  <span class=\"comment\"># (max_len, d_model)</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x</span>):</span><br><span class=\"line\">        <span class=\"comment\"># x: (B,S,D)</span></span><br><span class=\"line\">        S = x.size(<span class=\"number\">1</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> x + <span class=\"variable language_\">self</span>.pe[:S].unsqueeze(<span class=\"number\">0</span>)  <span class=\"comment\"># (1,S,D) 广播到 (B,S,D)</span></span><br></pre></td></tr></table></figure>\n\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>现代替代：RoPE、ALiBi 等（不展开）。</p></blockquote>\n<hr>\n<h2 id=\"6-编码器层（Encoder-Layer）\"><a href=\"#6-编码器层（Encoder-Layer）\" class=\"headerlink\" title=\"6) 编码器层（Encoder Layer）\"></a>6) 编码器层（Encoder Layer）</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">EncoderLayer</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, d_model, num_heads, d_ff, dropout=<span class=\"number\">0.1</span>, pre_norm=<span class=\"literal\">True</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.self_attn = MultiHeadAttention(d_model, num_heads, dropout)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.ffn = PositionwiseFFN(d_model, d_ff, dropout)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.res_attn = ResidualBlock(d_model, <span class=\"variable language_\">self</span>._attn, dropout, pre_norm)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.res_ffn  = ResidualBlock(d_model, <span class=\"variable language_\">self</span>.ffn,  dropout, pre_norm)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">_attn</span>(<span class=\"params\">self, x, attn_mask=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">        y, _ = <span class=\"variable language_\">self</span>.self_attn(x, x, attn_mask=attn_mask)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> y</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x, attn_mask=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">        x = <span class=\"variable language_\">self</span>.res_attn(x, attn_mask=attn_mask)</span><br><span class=\"line\">        x = <span class=\"variable language_\">self</span>.res_ffn(x)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> x</span><br></pre></td></tr></table></figure>\n\n<p><strong>自注意力 mask（padding mask）构造</strong>：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">make_padding_mask</span>(<span class=\"params\">pad_idx, tokens</span>):</span><br><span class=\"line\">    <span class=\"comment\"># tokens: (B,S) 的词 ID，pad 位置为 pad_idx</span></span><br><span class=\"line\">    <span class=\"comment\"># 返回 1 表示可见，0 表示屏蔽</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> (tokens != pad_idx).unsqueeze(<span class=\"number\">1</span>).unsqueeze(<span class=\"number\">1</span>)  <span class=\"comment\"># (B,1,1,S) 广播</span></span><br></pre></td></tr></table></figure>\n\n<hr>\n<h2 id=\"7-解码器层（Decoder-Layer）\"><a href=\"#7-解码器层（Decoder-Layer）\" class=\"headerlink\" title=\"7) 解码器层（Decoder Layer）\"></a>7) 解码器层（Decoder Layer）</h2><p>包含：<strong>Masked Self-Attn</strong>（因果遮蔽）、<strong>Cross-Attn</strong>（Q 来自 decoder，K&#x2F;V 来自 encoder）、<strong>FFN</strong>。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">make_causal_mask</span>(<span class=\"params\">S</span>):</span><br><span class=\"line\">    <span class=\"comment\"># (1,1,S,S) 下三角为 1（可见），上三角为 0（屏蔽）</span></span><br><span class=\"line\">    mask = torch.tril(torch.ones(S, S, dtype=torch.uint8))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> mask.view(<span class=\"number\">1</span>,<span class=\"number\">1</span>,S,S)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">DecoderLayer</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, d_model, num_heads, d_ff, dropout=<span class=\"number\">0.1</span>, pre_norm=<span class=\"literal\">True</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.self_attn = MultiHeadAttention(d_model, num_heads, dropout)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.cross_attn = MultiHeadAttention(d_model, num_heads, dropout)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.ffn = PositionwiseFFN(d_model, d_ff, dropout)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.res_self  = ResidualBlock(d_model, <span class=\"variable language_\">self</span>._self_attn,  dropout, pre_norm)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.res_cross = ResidualBlock(d_model, <span class=\"variable language_\">self</span>._cross_attn, dropout, pre_norm)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.res_ffn   = ResidualBlock(d_model, <span class=\"variable language_\">self</span>.ffn,        dropout, pre_norm)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">_self_attn</span>(<span class=\"params\">self, x, self_mask=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">        y, _ = <span class=\"variable language_\">self</span>.self_attn(x, x, attn_mask=self_mask)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> y</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">_cross_attn</span>(<span class=\"params\">self, x, memory, mem_mask=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">        y, _ = <span class=\"variable language_\">self</span>.cross_attn(x, memory, attn_mask=mem_mask)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> y</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x, memory, self_mask=<span class=\"literal\">None</span>, mem_mask=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">        x = <span class=\"variable language_\">self</span>.res_self(x, self_mask)</span><br><span class=\"line\">        x = <span class=\"variable language_\">self</span>.res_cross(x, memory, mem_mask)</span><br><span class=\"line\">        x = <span class=\"variable language_\">self</span>.res_ffn(x)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> x</span><br></pre></td></tr></table></figure>\n\n<p><strong>注意</strong>：</p>\n<ul>\n<li>交叉注意力的 <code>attn_mask</code> 常用于 padding 屏蔽（对 encoder memory 的 K 维度）；</li>\n<li>自注意力要叠加 <strong>因果 mask ∧ padding mask</strong>。</li>\n</ul>\n<hr>\n<h2 id=\"8-最小可运行-Transformer（Encoder-Decoder）\"><a href=\"#8-最小可运行-Transformer（Encoder-Decoder）\" class=\"headerlink\" title=\"8) 最小可运行 Transformer（Encoder-Decoder）\"></a>8) 最小可运行 Transformer（Encoder-Decoder）</h2><p>下例仅展示骨架，省略词表、损失与训练循环。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">MiniTransformer</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, vocab_size, d_model=<span class=\"number\">512</span>, num_heads=<span class=\"number\">8</span>, d_ff=<span class=\"number\">2048</span>,</span></span><br><span class=\"line\"><span class=\"params\">                 num_enc_layers=<span class=\"number\">6</span>, num_dec_layers=<span class=\"number\">6</span>, dropout=<span class=\"number\">0.1</span>, pad_idx=<span class=\"number\">0</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.d_model = d_model</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.pad_idx = pad_idx</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.src_embed = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.tgt_embed = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.pos = SinusoidalPositionalEncoding(d_model)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.enc_layers = nn.ModuleList([</span><br><span class=\"line\">            EncoderLayer(d_model, num_heads, d_ff, dropout) <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_enc_layers)</span><br><span class=\"line\">        ])</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.dec_layers = nn.ModuleList([</span><br><span class=\"line\">            DecoderLayer(d_model, num_heads, d_ff, dropout) <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_dec_layers)</span><br><span class=\"line\">        ])</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.lm_head = nn.Linear(d_model, vocab_size)  <span class=\"comment\"># tied weights 可选：与 tgt_embed.weight 共享</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">encode</span>(<span class=\"params\">self, src_tokens</span>):</span><br><span class=\"line\">        x = <span class=\"variable language_\">self</span>.pos(<span class=\"variable language_\">self</span>.src_embed(src_tokens) * math.sqrt(<span class=\"variable language_\">self</span>.d_model))</span><br><span class=\"line\">        src_pad_mask = make_padding_mask(<span class=\"variable language_\">self</span>.pad_idx, src_tokens)  <span class=\"comment\"># (B,1,1,S_src)</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> layer <span class=\"keyword\">in</span> <span class=\"variable language_\">self</span>.enc_layers:</span><br><span class=\"line\">            x = layer(x, attn_mask=src_pad_mask)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> x, src_pad_mask</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">decode</span>(<span class=\"params\">self, tgt_tokens, memory, src_pad_mask</span>):</span><br><span class=\"line\">        x = <span class=\"variable language_\">self</span>.pos(<span class=\"variable language_\">self</span>.tgt_embed(tgt_tokens) * math.sqrt(<span class=\"variable language_\">self</span>.d_model))</span><br><span class=\"line\">        B, S_t = tgt_tokens.shape</span><br><span class=\"line\">        causal = make_causal_mask(S_t).to(tgt_tokens.device)       <span class=\"comment\"># (1,1,S_t,S_t)</span></span><br><span class=\"line\">        tgt_pad_mask = make_padding_mask(<span class=\"variable language_\">self</span>.pad_idx, tgt_tokens) <span class=\"comment\"># (B,1,1,S_t)</span></span><br><span class=\"line\">        self_mask = causal &amp; tgt_pad_mask                          <span class=\"comment\"># 逻辑与</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> layer <span class=\"keyword\">in</span> <span class=\"variable language_\">self</span>.dec_layers:</span><br><span class=\"line\">            x = layer(x, memory, self_mask, src_pad_mask)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> x</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, src_tokens, tgt_tokens</span>):</span><br><span class=\"line\">        memory, src_pad_mask = <span class=\"variable language_\">self</span>.encode(src_tokens)</span><br><span class=\"line\">        dec_out = <span class=\"variable language_\">self</span>.decode(tgt_tokens, memory, src_pad_mask)</span><br><span class=\"line\">        logits = <span class=\"variable language_\">self</span>.lm_head(dec_out)  <span class=\"comment\"># (B,S_t,V)</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> logits</span><br></pre></td></tr></table></figure>\n\n<p><strong>贪心生成（示例）</strong>：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@torch.no_grad()</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">greedy_decode</span>(<span class=\"params\">model, src_tokens, bos_id, eos_id, max_len=<span class=\"number\">128</span></span>):</span><br><span class=\"line\">    device = src_tokens.device</span><br><span class=\"line\">    memory, src_pad_mask = model.encode(src_tokens)</span><br><span class=\"line\">    B = src_tokens.size(<span class=\"number\">0</span>)</span><br><span class=\"line\">    ys = torch.full((B,<span class=\"number\">1</span>), bos_id, dtype=torch.long, device=device)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(max_len-<span class=\"number\">1</span>):</span><br><span class=\"line\">        dec_out = model.decode(ys, memory, src_pad_mask)   <span class=\"comment\"># (B,S,D)</span></span><br><span class=\"line\">        next_logit = model.lm_head(dec_out[:, -<span class=\"number\">1</span>, :])      <span class=\"comment\"># (B,V)</span></span><br><span class=\"line\">        next_token = next_logit.argmax(-<span class=\"number\">1</span>, keepdim=<span class=\"literal\">True</span>)   <span class=\"comment\"># (B,1)</span></span><br><span class=\"line\">        ys = torch.cat([ys, next_token], dim=<span class=\"number\">1</span>)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (next_token == eos_id).<span class=\"built_in\">all</span>(): <span class=\"keyword\">break</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> ys</span><br></pre></td></tr></table></figure>\n\n<hr>\n<h2 id=\"9-复杂度与工程优化\"><a href=\"#9-复杂度与工程优化\" class=\"headerlink\" title=\"9) 复杂度与工程优化\"></a>9) 复杂度与工程优化</h2><ul>\n<li>注意力复杂度 <code>O(S^2·D)</code>，内存 <code>O(S^2)</code>；长序列瓶颈明显；</li>\n<li>典型优化：FlashAttention、块稀疏&#x2F;滑窗注意力、低秩核近似、KV Cache 等；</li>\n<li>训练 trick：Label Smoothing、学习率 warmup、梯度裁剪、Dropout&#x2F;Attention Dropout、权重共享（tied embeddings）。</li>\n</ul>\n<hr>\n<h2 id=\"10-常见坑位清单\"><a href=\"#10-常见坑位清单\" class=\"headerlink\" title=\"10) 常见坑位清单\"></a>10) 常见坑位清单</h2><ol>\n<li><strong>mask 方向&#x2F;形状</strong>：应能广播到 <code>(B,h,S_q,S_k)</code>；注意 batch-first；  </li>\n<li><strong>dtype</strong>：mask 多用 <code>bool</code>&#x2F;<code>uint8</code>；被屏蔽处加 <code>-inf</code> 前需确保 dtype 是浮点；  </li>\n<li><strong>全屏蔽→NaN</strong>：确保每个查询位置至少有一个可见键；  </li>\n<li><strong>缩放因子</strong>：别忘了 <code>/sqrt(d_k)</code>；  </li>\n<li><strong>Pre-LN &#x2F; Post-LN 混用</strong>：训练不稳定时优先用 Pre-LN；  </li>\n<li><strong>位置编码尺度</strong>：将嵌入乘 <code>sqrt(d_model)</code> 再相加位置编码，数值更稳定；  </li>\n<li><strong>batch-first 与官方 API</strong>：<code>nn.Transformer</code> 默认 seq-first，注意对齐；  </li>\n<li><strong>KV Cache</strong>（推理）：解码增量生成要缓存 past K&#x2F;V，避免二次方重复计算。</li>\n</ol>\n<hr>\n<h2 id=\"11-参考实现最小依赖清单\"><a href=\"#11-参考实现最小依赖清单\" class=\"headerlink\" title=\"11) 参考实现最小依赖清单\"></a>11) 参考实现最小依赖清单</h2><ul>\n<li>Python ≥ 3.8  </li>\n<li>PyTorch ≥ 1.12（支持 batch-first 层归一化与常规算子即可）</li>\n</ul>\n<hr>\n<h2 id=\"12-致谢与参考\"><a href=\"#12-致谢与参考\" class=\"headerlink\" title=\"12) 致谢与参考\"></a>12) 致谢与参考</h2><ul>\n<li>Vaswani et al., <em>Attention Is All You Need</em>, NeurIPS 2017.  </li>\n<li>PyTorch 文档：<code>nn.MultiheadAttention</code>, <code>nn.Transformer</code>。  </li>\n<li>相关工程优化可参考：FlashAttention、RoPE&#x2F;ALiBi&#x2F;相对位置编码等论文与实现。</li>\n</ul>\n<hr>\n","feature":true,"text":" 读者对象：已具备基本深度学习与 PyTorch 基础，希望系统掌握 Transformer 各模块设计与实现的工程师/学生。文章目标：从实现角度深入讲清楚每个...","permalink":"/post/transformer","photos":[],"count_time":{"symbolsCount":"12k","symbolsTime":"11 mins."},"categories":[{"name":"Algorithm","slug":"Algorithm","count":2,"path":"api/categories/Algorithm.json"}],"tags":[{"name":"code","slug":"code","count":2,"path":"api/tags/code.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%80%BB%E8%A7%88%EF%BC%88%E5%BD%A2%E7%8A%B6%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%B5%81%EF%BC%89\"><span class=\"toc-text\">总览（形状与数据流）</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#1-Scaled-Dot-Product-Attention%EF%BC%88%E7%BC%A9%E6%94%BE%E7%82%B9%E7%A7%AF%E6%B3%A8%E6%84%8F%E5%8A%9B%EF%BC%89\"><span class=\"toc-text\">1) Scaled Dot-Product Attention（缩放点积注意力）</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#2-Multi-Head-Attention%EF%BC%88%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B%EF%BC%89\"><span class=\"toc-text\">2) Multi-Head Attention（多头注意力）</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#3-%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5%E4%B8%8E-LayerNorm%EF%BC%88Pre-LN-vs-Post-LN%EF%BC%89\"><span class=\"toc-text\">3) 残差连接与 LayerNorm（Pre-LN vs Post-LN）</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#4-Position-wise-FFN%EF%BC%88%E9%80%90%E4%BD%8D%E7%BD%AE%E5%89%8D%E9%A6%88%E7%BD%91%E7%BB%9C%EF%BC%89\"><span class=\"toc-text\">4) Position-wise FFN（逐位置前馈网络）</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#5-%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%EF%BC%88Positional-Encoding%EF%BC%89\"><span class=\"toc-text\">5) 位置编码（Positional Encoding）</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#6-%E7%BC%96%E7%A0%81%E5%99%A8%E5%B1%82%EF%BC%88Encoder-Layer%EF%BC%89\"><span class=\"toc-text\">6) 编码器层（Encoder Layer）</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#7-%E8%A7%A3%E7%A0%81%E5%99%A8%E5%B1%82%EF%BC%88Decoder-Layer%EF%BC%89\"><span class=\"toc-text\">7) 解码器层（Decoder Layer）</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#8-%E6%9C%80%E5%B0%8F%E5%8F%AF%E8%BF%90%E8%A1%8C-Transformer%EF%BC%88Encoder-Decoder%EF%BC%89\"><span class=\"toc-text\">8) 最小可运行 Transformer（Encoder-Decoder）</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#9-%E5%A4%8D%E6%9D%82%E5%BA%A6%E4%B8%8E%E5%B7%A5%E7%A8%8B%E4%BC%98%E5%8C%96\"><span class=\"toc-text\">9) 复杂度与工程优化</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#10-%E5%B8%B8%E8%A7%81%E5%9D%91%E4%BD%8D%E6%B8%85%E5%8D%95\"><span class=\"toc-text\">10) 常见坑位清单</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#11-%E5%8F%82%E8%80%83%E5%AE%9E%E7%8E%B0%E6%9C%80%E5%B0%8F%E4%BE%9D%E8%B5%96%E6%B8%85%E5%8D%95\"><span class=\"toc-text\">11) 参考实现最小依赖清单</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#12-%E8%87%B4%E8%B0%A2%E4%B8%8E%E5%8F%82%E8%80%83\"><span class=\"toc-text\">12) 致谢与参考</span></a></li></ol>","author":{"name":"Rockway","slug":"blog-author","avatar":"https://lingmafuture.github.io/images/avatar.jpg","link":"https://lingmafuture.github.io","description":"Think like an artist, code like an artisan.","socials":{}},"mapped":true,"hidden":false,"prev_post":{"title":"Python常用数据处理库概览","uid":"d18fcd6dc1394e0e934c5a6ec63149f0","slug":"python-libraries","date":"2025-08-14T16:00:00.000Z","updated":"2025-08-15T11:58:07.409Z","comments":true,"path":"api/articles/python-libraries.json","keywords":"blog, technology, programming","cover":"https://raw.githubusercontent.com/LingmaFuture/lingmafuture.github.io/refs/heads/main/images/7.png","text":"引言在数据科学、数据工程和数据分析领域，数据处理是必不可少的基础环节。业界常说数据科学家将大部分时间花在整理清洗数据上，有经验的分析师都深知这一点：整个数据分析...","permalink":"/post/python-libraries","photos":[],"count_time":{"symbolsCount":"12k","symbolsTime":"11 mins."},"categories":[{"name":"Tech","slug":"Tech","count":2,"path":"api/categories/Tech.json"}],"tags":[{"name":"python","slug":"python","count":1,"path":"api/tags/python.json"}],"author":{"name":"Rockway","slug":"blog-author","avatar":"https://lingmafuture.github.io/images/avatar.jpg","link":"https://lingmafuture.github.io","description":"Think like an artist, code like an artisan.","socials":{}},"feature":true},"next_post":{"title":"Hexo 博客搭建指南：Aurora 主题与 Cloudflare + GitHub Pages 部署","uid":"7c571cda647bd463004b1d6057d46f3c","slug":"hexo-aurora","date":"2025-07-17T16:00:00.000Z","updated":"2025-08-15T12:06:51.506Z","comments":true,"path":"api/articles/hexo-aurora.json","keywords":"blog, technology, programming","cover":"https://raw.githubusercontent.com/LingmaFuture/lingmafuture.github.io/refs/heads/main/images/4.png","text":"Hexo 是一个基于 Node.js 的快速、高效的静态博客框架。通过 Hexo，我们可以使用 Markdown 编写文章，几秒钟内生成静态网页并部署到托管服务...","permalink":"/post/hexo-aurora","photos":[],"count_time":{"symbolsCount":"18k","symbolsTime":"16 mins."},"categories":[{"name":"Tech","slug":"Tech","count":2,"path":"api/categories/Tech.json"}],"tags":[{"name":"hexo","slug":"hexo","count":1,"path":"api/tags/hexo.json"}],"author":{"name":"Rockway","slug":"blog-author","avatar":"https://lingmafuture.github.io/images/avatar.jpg","link":"https://lingmafuture.github.io","description":"Think like an artist, code like an artisan.","socials":{}},"feature":true}}