{"title":"Transformer 架构详解（含 PyTorch 代码）","uid":"48a8ab6165265f9c75721ce750feca94","slug":"transformer","date":"2025-08-12T16:00:00.000Z","updated":"2025-08-23T01:50:17.699Z","comments":true,"path":"api/articles/transformer.json","keywords":"blog, technology, programming","cover":"images/transformer.png","content":"<p>读者对象：已具备基本深度学习与 PyTorch 基础，希望系统掌握 Transformer\r\n各模块设计与实现的工程师/学生。<br>\r\n文章目标：<strong>从实现角度深入讲清楚每个子模块</strong>（Scaled\r\nDot-Product Attention、多头注意力、残差连接+LayerNorm、Position-wise\r\nFFN、位置编码、编码器/解码器层与整模型），并给出<strong>可运行的最小化\r\nPyTorch 参考实现</strong>与常见坑位排查清单。<br>\r\n题外话：背景演进（RNN/Conv）不展开，直接上核心。</p>\r\n<h2 id=\"总览形状与数据流\">总览（形状与数据流）</h2>\r\n<p>我们统一使用 <strong>batch-first</strong> 约定：张量形状均为\r\n<code>(B, S, D)</code>。</p>\r\n<ul>\r\n<li><code>B</code>：batch size<br>\r\n</li>\r\n<li><code>S</code>：序列长度（<code>S_q</code>/<code>S_k</code>/<code>S_v</code>\r\n在解码器交叉注意力中可能不同）<br>\r\n</li>\r\n<li><code>D</code>：模型隐藏维度 <code>d_model</code></li>\r\n</ul>\r\n<p>ASCII 拆解：</p>\r\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[Token Embedding] + [Positional Encoding]</span><br><span class=\"line\">          │ (B,S,D)</span><br><span class=\"line\">          ▼</span><br><span class=\"line\">   ┌───────────── Encoder Layer × N ─────────────┐</span><br><span class=\"line\">   │  Self-Attn ── Add&amp;Norm ── FFN ── Add&amp;Norm   │</span><br><span class=\"line\">   └──────────────────────────────────────────────┘</span><br><span class=\"line\">          │ (B,S,D) = Encoder Memory (Keys/Values)</span><br><span class=\"line\">          ▼</span><br><span class=\"line\">   ┌───────────── Decoder Layer × N ─────────────┐</span><br><span class=\"line\">   │  Masked Self-Attn ─ Add&amp;Norm                │</span><br><span class=\"line\">   │  Cross Attn (Q=decoder, K,V=encoder)        │</span><br><span class=\"line\">   │               ─ Add&amp;Norm ─ FFN ─ Add&amp;Norm   │</span><br><span class=\"line\">   └──────────────────────────────────────────────┘</span><br><span class=\"line\">          │ (B,S,D)</span><br><span class=\"line\">          ▼</span><br><span class=\"line\">      [Linear → Softmax] → 生成下一个 token</span><br></pre></td></tr></table></figure>\r\n<h2 id=\"scaled-dot-product-attention缩放点积注意力\">1) Scaled\r\nDot-Product Attention（缩放点积注意力）</h2>\r\n<p><strong>公式</strong>：</p>\r\n<p><span class=\"math display\"><mjx-container class=\"MathJax\" jax=\"SVG\" display=\"true\"><svg style=\"vertical-align: -2.308ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"46.313ex\" height=\"5.727ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -1511.3 20470.1 2531.3\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"TeXAtom\" data-mjx-texclass=\"ORD\"><g data-mml-node=\"mi\"><path data-c=\"41\" d=\"M255 0Q240 3 140 3Q48 3 39 0H32V46H47Q119 49 139 88Q140 91 192 245T295 553T348 708Q351 716 366 716H376Q396 715 400 709Q402 707 508 390L617 67Q624 54 636 51T687 46H717V0H708Q699 3 581 3Q458 3 437 0H427V46H440Q510 46 510 64Q510 66 486 138L462 209H229L209 150Q189 91 189 85Q189 72 209 59T259 46H264V0H255ZM447 255L345 557L244 256Q244 255 345 255H447Z\"></path><path data-c=\"74\" d=\"M27 422Q80 426 109 478T141 600V615H181V431H316V385H181V241Q182 116 182 100T189 68Q203 29 238 29Q282 29 292 100Q293 108 293 146V181H333V146V134Q333 57 291 17Q264 -10 221 -10Q187 -10 162 2T124 33T105 68T98 100Q97 107 97 248V385H18V422H27Z\" transform=\"translate(750,0)\"></path><path data-c=\"74\" d=\"M27 422Q80 426 109 478T141 600V615H181V431H316V385H181V241Q182 116 182 100T189 68Q203 29 238 29Q282 29 292 100Q293 108 293 146V181H333V146V134Q333 57 291 17Q264 -10 221 -10Q187 -10 162 2T124 33T105 68T98 100Q97 107 97 248V385H18V422H27Z\" transform=\"translate(1139,0)\"></path><path data-c=\"65\" d=\"M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z\" transform=\"translate(1528,0)\"></path><path data-c=\"6E\" d=\"M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z\" transform=\"translate(1972,0)\"></path><path data-c=\"74\" d=\"M27 422Q80 426 109 478T141 600V615H181V431H316V385H181V241Q182 116 182 100T189 68Q203 29 238 29Q282 29 292 100Q293 108 293 146V181H333V146V134Q333 57 291 17Q264 -10 221 -10Q187 -10 162 2T124 33T105 68T98 100Q97 107 97 248V385H18V422H27Z\" transform=\"translate(2528,0)\"></path><path data-c=\"69\" d=\"M69 609Q69 637 87 653T131 669Q154 667 171 652T188 609Q188 579 171 564T129 549Q104 549 87 564T69 609ZM247 0Q232 3 143 3Q132 3 106 3T56 1L34 0H26V46H42Q70 46 91 49Q100 53 102 60T104 102V205V293Q104 345 102 359T88 378Q74 385 41 385H30V408Q30 431 32 431L42 432Q52 433 70 434T106 436Q123 437 142 438T171 441T182 442H185V62Q190 52 197 50T232 46H255V0H247Z\" transform=\"translate(2917,0)\"></path><path data-c=\"6F\" d=\"M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z\" transform=\"translate(3195,0)\"></path><path data-c=\"6E\" d=\"M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z\" transform=\"translate(3695,0)\"></path></g></g><g data-mml-node=\"mo\" transform=\"translate(4251,0)\"><path data-c=\"28\" d=\"M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(4640,0)\"><path data-c=\"1D444\" d=\"M399 -80Q399 -47 400 -30T402 -11V-7L387 -11Q341 -22 303 -22Q208 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435Q740 255 592 107Q529 47 461 16L444 8V3Q444 2 449 -24T470 -66T516 -82Q551 -82 583 -60T625 -3Q631 11 638 11Q647 11 649 2Q649 -6 639 -34T611 -100T557 -165T481 -194Q399 -194 399 -87V-80ZM636 468Q636 523 621 564T580 625T530 655T477 665Q429 665 379 640Q277 591 215 464T153 216Q153 110 207 59Q231 38 236 38V46Q236 86 269 120T347 155Q372 155 390 144T417 114T429 82T435 55L448 64Q512 108 557 185T619 334T636 468ZM314 18Q362 18 404 39L403 49Q399 104 366 115Q354 117 347 117Q344 117 341 117T337 118Q317 118 296 98T274 52Q274 18 314 18Z\"></path></g><g data-mml-node=\"mo\" transform=\"translate(5431,0)\"><path data-c=\"2C\" d=\"M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(5875.7,0)\"><path data-c=\"1D43E\" d=\"M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z\"></path></g><g data-mml-node=\"mo\" transform=\"translate(6764.7,0)\"><path data-c=\"2C\" d=\"M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(7209.3,0)\"><path data-c=\"1D449\" d=\"M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z\"></path></g><g data-mml-node=\"mo\" transform=\"translate(7978.3,0)\"><path data-c=\"29\" d=\"M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z\"></path></g><g data-mml-node=\"mo\" transform=\"translate(8645.1,0)\"><path data-c=\"3D\" d=\"M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z\"></path></g><g data-mml-node=\"TeXAtom\" data-mjx-texclass=\"ORD\" transform=\"translate(9700.9,0)\"><g data-mml-node=\"mi\"><path data-c=\"73\" d=\"M295 316Q295 356 268 385T190 414Q154 414 128 401Q98 382 98 349Q97 344 98 336T114 312T157 287Q175 282 201 278T245 269T277 256Q294 248 310 236T342 195T359 133Q359 71 321 31T198 -10H190Q138 -10 94 26L86 19L77 10Q71 4 65 -1L54 -11H46H42Q39 -11 33 -5V74V132Q33 153 35 157T45 162H54Q66 162 70 158T75 146T82 119T101 77Q136 26 198 26Q295 26 295 104Q295 133 277 151Q257 175 194 187T111 210Q75 227 54 256T33 318Q33 357 50 384T93 424T143 442T187 447H198Q238 447 268 432L283 424L292 431Q302 440 314 448H322H326Q329 448 335 442V310L329 304H301Q295 310 295 316Z\"></path><path data-c=\"6F\" d=\"M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z\" transform=\"translate(394,0)\"></path><path data-c=\"66\" d=\"M273 0Q255 3 146 3Q43 3 34 0H26V46H42Q70 46 91 49Q99 52 103 60Q104 62 104 224V385H33V431H104V497L105 564L107 574Q126 639 171 668T266 704Q267 704 275 704T289 705Q330 702 351 679T372 627Q372 604 358 590T321 576T284 590T270 627Q270 647 288 667H284Q280 668 273 668Q245 668 223 647T189 592Q183 572 182 497V431H293V385H185V225Q185 63 186 61T189 57T194 54T199 51T206 49T213 48T222 47T231 47T241 46T251 46H282V0H273Z\" transform=\"translate(894,0)\"></path><path data-c=\"74\" d=\"M27 422Q80 426 109 478T141 600V615H181V431H316V385H181V241Q182 116 182 100T189 68Q203 29 238 29Q282 29 292 100Q293 108 293 146V181H333V146V134Q333 57 291 17Q264 -10 221 -10Q187 -10 162 2T124 33T105 68T98 100Q97 107 97 248V385H18V422H27Z\" transform=\"translate(1200,0)\"></path><path data-c=\"6D\" d=\"M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z\" transform=\"translate(1589,0)\"></path><path data-c=\"61\" d=\"M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z\" transform=\"translate(2422,0)\"></path><path data-c=\"78\" d=\"M201 0Q189 3 102 3Q26 3 17 0H11V46H25Q48 47 67 52T96 61T121 78T139 96T160 122T180 150L226 210L168 288Q159 301 149 315T133 336T122 351T113 363T107 370T100 376T94 379T88 381T80 383Q74 383 44 385H16V431H23Q59 429 126 429Q219 429 229 431H237V385Q201 381 201 369Q201 367 211 353T239 315T268 274L272 270L297 304Q329 345 329 358Q329 364 327 369T322 376T317 380T310 384L307 385H302V431H309Q324 428 408 428Q487 428 493 431H499V385H492Q443 385 411 368Q394 360 377 341T312 257L296 236L358 151Q424 61 429 57T446 50Q464 46 499 46H516V0H510H502Q494 1 482 1T457 2T432 2T414 3Q403 3 377 3T327 1L304 0H295V46H298Q309 46 320 51T331 63Q331 65 291 120L250 175Q249 174 219 133T185 88Q181 83 181 74Q181 63 188 55T206 46Q208 46 208 23V0H201Z\" transform=\"translate(2922,0)\"></path></g></g><g data-mml-node=\"mstyle\" transform=\"translate(13150.9,0)\"><g data-mml-node=\"mspace\"></g></g><g data-mml-node=\"mrow\" transform=\"translate(13150.6,0)\"><g data-mml-node=\"mo\" transform=\"translate(0 -0.5)\"><path data-c=\"28\" d=\"M701 -940Q701 -943 695 -949H664Q662 -947 636 -922T591 -879T537 -818T475 -737T412 -636T350 -511T295 -362T250 -186T221 17T209 251Q209 962 573 1361Q596 1386 616 1405T649 1437T664 1450H695Q701 1444 701 1441Q701 1436 681 1415T629 1356T557 1261T476 1118T400 927T340 675T308 359Q306 321 306 250Q306 -139 400 -430T690 -924Q701 -936 701 -940Z\"></path></g><g data-mml-node=\"mfrac\" transform=\"translate(736,0)\"><g data-mml-node=\"mrow\" transform=\"translate(220,676)\"><g data-mml-node=\"mi\"><path data-c=\"1D444\" d=\"M399 -80Q399 -47 400 -30T402 -11V-7L387 -11Q341 -22 303 -22Q208 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435Q740 255 592 107Q529 47 461 16L444 8V3Q444 2 449 -24T470 -66T516 -82Q551 -82 583 -60T625 -3Q631 11 638 11Q647 11 649 2Q649 -6 639 -34T611 -100T557 -165T481 -194Q399 -194 399 -87V-80ZM636 468Q636 523 621 564T580 625T530 655T477 665Q429 665 379 640Q277 591 215 464T153 216Q153 110 207 59Q231 38 236 38V46Q236 86 269 120T347 155Q372 155 390 144T417 114T429 82T435 55L448 64Q512 108 557 185T619 334T636 468ZM314 18Q362 18 404 39L403 49Q399 104 366 115Q354 117 347 117Q344 117 341 117T337 118Q317 118 296 98T274 52Q274 18 314 18Z\"></path></g><g data-mml-node=\"msup\" transform=\"translate(791,0)\"><g data-mml-node=\"mi\"><path data-c=\"1D43E\" d=\"M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(974,363) scale(0.707)\"><path data-c=\"22A4\" d=\"M55 642T55 648T59 659T66 666T71 668H708Q723 660 723 648T708 628H409V15Q402 2 391 0Q387 0 384 1T379 3T375 6T373 9T371 13T369 16V628H71Q70 628 67 630T59 637Z\"></path></g></g></g><g data-mml-node=\"msqrt\" transform=\"translate(490.4,-855.6)\"><g transform=\"translate(853,0)\"><g data-mml-node=\"msub\"><g data-mml-node=\"mi\"><path data-c=\"1D451\" d=\"M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(553,-150) scale(0.707)\"><path data-c=\"1D458\" d=\"M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z\"></path></g></g></g><g data-mml-node=\"mo\" transform=\"translate(0,35.6)\"><path data-c=\"221A\" d=\"M95 178Q89 178 81 186T72 200T103 230T169 280T207 309Q209 311 212 311H213Q219 311 227 294T281 177Q300 134 312 108L397 -77Q398 -77 501 136T707 565T814 786Q820 800 834 800Q841 800 846 794T853 782V776L620 293L385 -193Q381 -200 366 -200Q357 -200 354 -197Q352 -195 256 15L160 225L144 214Q129 202 113 190T95 178Z\"></path></g><rect width=\"971.4\" height=\"60\" x=\"853\" y=\"775.6\"></rect></g><rect width=\"2565.1\" height=\"60\" x=\"120\" y=\"220\"></rect></g><g data-mml-node=\"mo\" transform=\"translate(3763.4,0)\"><path data-c=\"2B\" d=\"M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(4763.6,0)\"><path data-c=\"1D440\" d=\"M289 629Q289 635 232 637Q208 637 201 638T194 648Q194 649 196 659Q197 662 198 666T199 671T201 676T203 679T207 681T212 683T220 683T232 684Q238 684 262 684T307 683Q386 683 398 683T414 678Q415 674 451 396L487 117L510 154Q534 190 574 254T662 394Q837 673 839 675Q840 676 842 678T846 681L852 683H948Q965 683 988 683T1017 684Q1051 684 1051 673Q1051 668 1048 656T1045 643Q1041 637 1008 637Q968 636 957 634T939 623Q936 618 867 340T797 59Q797 55 798 54T805 50T822 48T855 46H886Q892 37 892 35Q892 19 885 5Q880 0 869 0Q864 0 828 1T736 2Q675 2 644 2T609 1Q592 1 592 11Q592 13 594 25Q598 41 602 43T625 46Q652 46 685 49Q699 52 704 61Q706 65 742 207T813 490T848 631L654 322Q458 10 453 5Q451 4 449 3Q444 0 433 0Q418 0 415 7Q413 11 374 317L335 624L267 354Q200 88 200 79Q206 46 272 46H282Q288 41 289 37T286 19Q282 3 278 1Q274 0 267 0Q265 0 255 0T221 1T157 2Q127 2 95 1T58 0Q43 0 39 2T35 11Q35 13 38 25T43 40Q45 46 65 46Q135 46 154 86Q158 92 223 354T289 629Z\"></path></g><g data-mml-node=\"mo\" transform=\"translate(5814.6,0) translate(0 -0.5)\"><path data-c=\"29\" d=\"M34 1438Q34 1446 37 1448T50 1450H56H71Q73 1448 99 1423T144 1380T198 1319T260 1238T323 1137T385 1013T440 864T485 688T514 485T526 251Q526 134 519 53Q472 -519 162 -860Q139 -885 119 -904T86 -936T71 -949H56Q43 -949 39 -947T34 -937Q88 -883 140 -813Q428 -430 428 251Q428 453 402 628T338 922T245 1146T145 1309T46 1425Q44 1427 42 1429T39 1433T36 1436L34 1438Z\"></path></g></g><g data-mml-node=\"mi\" transform=\"translate(19701.1,0)\"><path data-c=\"1D449\" d=\"M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z\"></path></g></g></g></svg></mjx-container></span></p>\r\n<p><span class=\"math display\"><mjx-container class=\"MathJax\" jax=\"SVG\" display=\"true\"><svg style=\"vertical-align: -0.439ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"53.36ex\" height=\"2.501ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -911.5 23585.2 1105.5\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"mi\"><path data-c=\"1D444\" d=\"M399 -80Q399 -47 400 -30T402 -11V-7L387 -11Q341 -22 303 -22Q208 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435Q740 255 592 107Q529 47 461 16L444 8V3Q444 2 449 -24T470 -66T516 -82Q551 -82 583 -60T625 -3Q631 11 638 11Q647 11 649 2Q649 -6 639 -34T611 -100T557 -165T481 -194Q399 -194 399 -87V-80ZM636 468Q636 523 621 564T580 625T530 655T477 665Q429 665 379 640Q277 591 215 464T153 216Q153 110 207 59Q231 38 236 38V46Q236 86 269 120T347 155Q372 155 390 144T417 114T429 82T435 55L448 64Q512 108 557 185T619 334T636 468ZM314 18Q362 18 404 39L403 49Q399 104 366 115Q354 117 347 117Q344 117 341 117T337 118Q317 118 296 98T274 52Q274 18 314 18Z\"></path></g><g data-mml-node=\"mo\" transform=\"translate(1068.8,0)\"><path data-c=\"2208\" d=\"M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z\"></path></g><g data-mml-node=\"msup\" transform=\"translate(2013.6,0)\"><g data-mml-node=\"mi\"><path data-c=\"1D445\" d=\"M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z\"></path></g><g data-mml-node=\"TeXAtom\" transform=\"translate(792,413) scale(0.707)\" data-mjx-texclass=\"ORD\"><g data-mml-node=\"mi\"><path data-c=\"1D435\" d=\"M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z\"></path></g><g data-mml-node=\"mo\" transform=\"translate(759,0)\"><path data-c=\"D7\" d=\"M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(1537,0)\"><path data-c=\"210E\" d=\"M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z\"></path></g><g data-mml-node=\"mo\" transform=\"translate(2113,0)\"><path data-c=\"D7\" d=\"M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z\"></path></g><g data-mml-node=\"msub\" transform=\"translate(2891,0)\"><g data-mml-node=\"mi\"><path data-c=\"1D446\" d=\"M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(646,-150) scale(0.707)\"><path data-c=\"1D45E\" d=\"M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z\"></path></g></g><g data-mml-node=\"mo\" transform=\"translate(3912.3,0)\"><path data-c=\"D7\" d=\"M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z\"></path></g><g data-mml-node=\"msub\" transform=\"translate(4690.3,0)\"><g data-mml-node=\"mi\"><path data-c=\"1D451\" d=\"M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(553,-150) scale(0.707)\"><path data-c=\"1D458\" d=\"M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z\"></path></g></g></g></g><g data-mml-node=\"mo\" transform=\"translate(6859,0)\"><path data-c=\"2C\" d=\"M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z\"></path></g><g data-mml-node=\"mstyle\" transform=\"translate(7137,0)\"><g data-mml-node=\"mspace\"></g></g><g data-mml-node=\"mi\" transform=\"translate(8303.6,0)\"><path data-c=\"1D43E\" d=\"M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z\"></path></g><g data-mml-node=\"mo\" transform=\"translate(9470.4,0)\"><path data-c=\"2208\" d=\"M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z\"></path></g><g data-mml-node=\"msup\" transform=\"translate(10415.2,0)\"><g data-mml-node=\"mi\"><path data-c=\"1D445\" d=\"M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z\"></path></g><g data-mml-node=\"TeXAtom\" transform=\"translate(792,413) scale(0.707)\" data-mjx-texclass=\"ORD\"><g data-mml-node=\"mi\"><path data-c=\"1D435\" d=\"M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z\"></path></g><g data-mml-node=\"mo\" transform=\"translate(759,0)\"><path data-c=\"D7\" d=\"M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(1537,0)\"><path data-c=\"210E\" d=\"M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z\"></path></g><g data-mml-node=\"mo\" transform=\"translate(2113,0)\"><path data-c=\"D7\" d=\"M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z\"></path></g><g data-mml-node=\"msub\" transform=\"translate(2891,0)\"><g data-mml-node=\"mi\"><path data-c=\"1D446\" d=\"M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(646,-150) scale(0.707)\"><path data-c=\"1D458\" d=\"M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z\"></path></g></g><g data-mml-node=\"mo\" transform=\"translate(3955.4,0)\"><path data-c=\"D7\" d=\"M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z\"></path></g><g data-mml-node=\"msub\" transform=\"translate(4733.4,0)\"><g data-mml-node=\"mi\"><path data-c=\"1D451\" d=\"M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(553,-150) scale(0.707)\"><path data-c=\"1D458\" d=\"M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z\"></path></g></g></g></g><g data-mml-node=\"mo\" transform=\"translate(15291.1,0)\"><path data-c=\"2C\" d=\"M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z\"></path></g><g data-mml-node=\"mstyle\" transform=\"translate(15569.1,0)\"><g data-mml-node=\"mspace\"></g></g><g data-mml-node=\"mi\" transform=\"translate(16735.8,0)\"><path data-c=\"1D449\" d=\"M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z\"></path></g><g data-mml-node=\"mo\" transform=\"translate(17782.5,0)\"><path data-c=\"2208\" d=\"M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z\"></path></g><g data-mml-node=\"msup\" transform=\"translate(18727.3,0)\"><g data-mml-node=\"mi\"><path data-c=\"1D445\" d=\"M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z\"></path></g><g data-mml-node=\"TeXAtom\" transform=\"translate(792,413) scale(0.707)\" data-mjx-texclass=\"ORD\"><g data-mml-node=\"mi\"><path data-c=\"1D435\" d=\"M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z\"></path></g><g data-mml-node=\"mo\" transform=\"translate(759,0)\"><path data-c=\"D7\" d=\"M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(1537,0)\"><path data-c=\"210E\" d=\"M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z\"></path></g><g data-mml-node=\"mo\" transform=\"translate(2113,0)\"><path data-c=\"D7\" d=\"M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z\"></path></g><g data-mml-node=\"msub\" transform=\"translate(2891,0)\"><g data-mml-node=\"mi\"><path data-c=\"1D446\" d=\"M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(646,-150) scale(0.707)\"><path data-c=\"1D458\" d=\"M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z\"></path></g></g><g data-mml-node=\"mo\" transform=\"translate(3955.4,0)\"><path data-c=\"D7\" d=\"M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z\"></path></g><g data-mml-node=\"msub\" transform=\"translate(4733.4,0)\"><g data-mml-node=\"mi\"><path data-c=\"1D451\" d=\"M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(553,-150) scale(0.707)\"><path data-c=\"1D463\" d=\"M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z\"></path></g></g></g></g></g></g></svg></mjx-container></span></p>\r\n<ul>\r\n<li><code>h</code> 为注意力头数；常取 <span class=\"math inline\"><mjx-container class=\"MathJax\" jax=\"SVG\"><svg style=\"vertical-align: -0.566ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"18.405ex\" height=\"2.262ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -750 8135.2 1000\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"msub\"><g data-mml-node=\"mi\"><path data-c=\"1D451\" d=\"M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(553,-150) scale(0.707)\"><path data-c=\"1D458\" d=\"M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z\"></path></g></g><g data-mml-node=\"mo\" transform=\"translate(1249.2,0)\"><path data-c=\"3D\" d=\"M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z\"></path></g><g data-mml-node=\"msub\" transform=\"translate(2305,0)\"><g data-mml-node=\"mi\"><path data-c=\"1D451\" d=\"M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(553,-150) scale(0.707)\"><path data-c=\"1D463\" d=\"M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z\"></path></g></g><g data-mml-node=\"mo\" transform=\"translate(3528.7,0)\"><path data-c=\"3D\" d=\"M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z\"></path></g><g data-mml-node=\"msub\" transform=\"translate(4584.5,0)\"><g data-mml-node=\"mi\"><path data-c=\"1D451\" d=\"M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z\"></path></g><g data-mml-node=\"TeXAtom\" transform=\"translate(553,-150) scale(0.707)\" data-mjx-texclass=\"ORD\"><g data-mml-node=\"mi\"><path data-c=\"1D45A\" d=\"M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(878,0)\"><path data-c=\"1D45C\" d=\"M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(1363,0)\"><path data-c=\"1D451\" d=\"M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(1883,0)\"><path data-c=\"1D452\" d=\"M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(2349,0)\"><path data-c=\"1D459\" d=\"M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z\"></path></g></g></g><g data-mml-node=\"TeXAtom\" data-mjx-texclass=\"ORD\" transform=\"translate(7059.2,0)\"><g data-mml-node=\"mo\"><path data-c=\"2F\" d=\"M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z\"></path></g></g><g data-mml-node=\"mi\" transform=\"translate(7559.2,0)\"><path data-c=\"210E\" d=\"M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z\"></path></g></g></g></svg></mjx-container></span><br>\r\n</li>\r\n<li><code>M</code> 为掩码（mask），屏蔽无效或未来位置，形状可广播到\r\n<code>(B,h,S_q,S_k)</code>；被屏蔽处取 <code>-inf</code></li>\r\n</ul>\r\n<p><strong>实现要点</strong>：</p>\r\n<ul>\r\n<li>除以 <span class=\"math inline\"><mjx-container class=\"MathJax\" jax=\"SVG\"><svg style=\"vertical-align: -0.372ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"4.128ex\" height=\"2.398ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -895.6 1824.4 1060\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"msqrt\"><g transform=\"translate(853,0)\"><g data-mml-node=\"msub\"><g data-mml-node=\"mi\"><path data-c=\"1D451\" d=\"M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(553,-150) scale(0.707)\"><path data-c=\"1D458\" d=\"M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z\"></path></g></g></g><g data-mml-node=\"mo\" transform=\"translate(0,35.6)\"><path data-c=\"221A\" d=\"M95 178Q89 178 81 186T72 200T103 230T169 280T207 309Q209 311 212 311H213Q219 311 227 294T281 177Q300 134 312 108L397 -77Q398 -77 501 136T707 565T814 786Q820 800 834 800Q841 800 846 794T853 782V776L620 293L385 -193Q381 -200 366 -200Q357 -200 354 -197Q352 -195 256 15L160 225L144 214Q129 202 113 190T95 178Z\"></path></g><rect width=\"971.4\" height=\"60\" x=\"853\" y=\"775.6\"></rect></g></g></g></svg></mjx-container></span>\r\n防止内积随维度增大导致 softmax 梯度极小；</li>\r\n<li>掩码一定要在 softmax 之前加到 logits 上；</li>\r\n<li><strong>全被屏蔽</strong>会导致\r\n<code>softmax(-inf)=NaN</code>，推理/训练前要确保至少一处可见。</li>\r\n</ul>\r\n<p><strong>参考实现</strong>：</p>\r\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch, math</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn.functional <span class=\"keyword\">as</span> F</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">scaled_dot_product_attention</span>(<span class=\"params\">Q, K, V, mask=<span class=\"literal\">None</span>, dropout_p=<span class=\"number\">0.0</span></span>):</span><br><span class=\"line\">    <span class=\"comment\"># Q,K,V: (B,h,S,dk/dv)</span></span><br><span class=\"line\">    scores = Q @ K.transpose(-<span class=\"number\">2</span>, -<span class=\"number\">1</span>) / math.sqrt(Q.size(-<span class=\"number\">1</span>))  <span class=\"comment\"># (B,h,S_q,S_k)</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> mask <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">        <span class=\"comment\"># 要求 mask 可广播至 scores 形状；True/1 表示「可见」更直观的话可改写</span></span><br><span class=\"line\">        scores = scores.masked_fill(mask == <span class=\"number\">0</span>, <span class=\"built_in\">float</span>(<span class=\"string\">\"-inf\"</span>))</span><br><span class=\"line\">    attn = F.softmax(scores, dim=-<span class=\"number\">1</span>)            <span class=\"comment\"># (B,h,S_q,S_k)</span></span><br><span class=\"line\">    attn = F.dropout(attn, p=dropout_p, training=Q.requires_grad)</span><br><span class=\"line\">    out = attn @ V                              <span class=\"comment\"># (B,h,S_q,dv)</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> out, attn</span><br></pre></td></tr></table></figure>\r\n<h2 id=\"multi-head-attention多头注意力\">2) Multi-Head\r\nAttention（多头注意力）</h2>\r\n<p>核心思想：用多个线性投影将输入映射到不同子空间并行做注意力，然后拼接聚合，提高表达能力。</p>\r\n<p><strong>实现要点</strong>：</p>\r\n<ul>\r\n<li>线性层一次性生成 <code>Q,K,V</code>，再\r\n<code>view</code>/<code>reshape</code> 成 <span class=\"math inline\"><mjx-container class=\"MathJax\" jax=\"SVG\"><svg style=\"vertical-align: -0.566ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"11.456ex\" height=\"2.262ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -750 5063.4 1000\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"mo\"><path data-c=\"28\" d=\"M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(389,0)\"><path data-c=\"1D435\" d=\"M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z\"></path></g><g data-mml-node=\"mo\" transform=\"translate(1148,0)\"><path data-c=\"2C\" d=\"M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(1592.7,0)\"><path data-c=\"210E\" d=\"M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z\"></path></g><g data-mml-node=\"mo\" transform=\"translate(2168.7,0)\"><path data-c=\"2C\" d=\"M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(2613.3,0)\"><path data-c=\"1D446\" d=\"M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z\"></path></g><g data-mml-node=\"mo\" transform=\"translate(3258.3,0)\"><path data-c=\"2C\" d=\"M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z\"></path></g><g data-mml-node=\"msub\" transform=\"translate(3703,0)\"><g data-mml-node=\"mi\"><path data-c=\"1D451\" d=\"M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(553,-150) scale(0.707)\"><path data-c=\"1D458\" d=\"M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z\"></path></g></g><g data-mml-node=\"mo\" transform=\"translate(4674.4,0)\"><path data-c=\"29\" d=\"M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z\"></path></g></g></g></svg></mjx-container></span>；</li>\r\n<li>拼接时 <code>transpose+contiguous+view</code> 回到\r\n<code>(B,S,D)</code>；</li>\r\n<li>统一用 <strong>batch-first</strong>，不与 PyTorch 自带\r\n<code>nn.MultiheadAttention</code>（默认为 seq-first）混淆。</li>\r\n</ul>\r\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">MultiHeadAttention</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, d_model: <span class=\"built_in\">int</span>, num_heads: <span class=\"built_in\">int</span>, dropout: <span class=\"built_in\">float</span> = <span class=\"number\">0.0</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        <span class=\"keyword\">assert</span> d_model % num_heads == <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.d_model = d_model</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.h = num_heads</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.d_k = d_model // num_heads</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.qkv = nn.Linear(d_model, <span class=\"number\">3</span> * d_model, bias=<span class=\"literal\">False</span>)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.o_proj = nn.Linear(d_model, d_model, bias=<span class=\"literal\">False</span>)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.dropout = nn.Dropout(dropout)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x_q, x_kv, attn_mask=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">        <span class=\"comment\"># x_q: (B,S_q,D), x_kv: (B,S_k,D)</span></span><br><span class=\"line\">        B, Sq, _ = x_q.shape</span><br><span class=\"line\">        Sk = x_kv.shape[<span class=\"number\">1</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">        qkv_q = <span class=\"variable language_\">self</span>.qkv(x_q)                                  <span class=\"comment\"># (B,S_q,3D)</span></span><br><span class=\"line\">        qkv_kv = <span class=\"variable language_\">self</span>.qkv(x_kv)                                <span class=\"comment\"># 共享参数的简化实现</span></span><br><span class=\"line\">        Q = qkv_q[..., :<span class=\"variable language_\">self</span>.d_model]</span><br><span class=\"line\">        K = qkv_kv[..., <span class=\"variable language_\">self</span>.d_model:<span class=\"number\">2</span>*<span class=\"variable language_\">self</span>.d_model]</span><br><span class=\"line\">        V = qkv_kv[..., <span class=\"number\">2</span>*<span class=\"variable language_\">self</span>.d_model:]</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># (B,S,D) -&gt; (B,h,S,d_k)</span></span><br><span class=\"line\">        <span class=\"keyword\">def</span> <span class=\"title function_\">split_heads</span>(<span class=\"params\">t</span>):</span><br><span class=\"line\">            <span class=\"keyword\">return</span> t.view(B, -<span class=\"number\">1</span>, <span class=\"variable language_\">self</span>.h, <span class=\"variable language_\">self</span>.d_k).transpose(<span class=\"number\">1</span>, <span class=\"number\">2</span>).contiguous()</span><br><span class=\"line\">        Q, K, V = <span class=\"built_in\">map</span>(split_heads, (Q, K, V))</span><br><span class=\"line\"></span><br><span class=\"line\">        out, attn = scaled_dot_product_attention(Q, K, V, mask=attn_mask, dropout_p=<span class=\"variable language_\">self</span>.dropout.p)</span><br><span class=\"line\">        <span class=\"comment\"># $(B,h,S_q,d_k)$ -&gt; $(B,S_q,D)$</span></span><br><span class=\"line\">        out = out.transpose(<span class=\"number\">1</span>, <span class=\"number\">2</span>).contiguous().view(B, Sq, <span class=\"variable language_\">self</span>.d_model)</span><br><span class=\"line\">        out = <span class=\"variable language_\">self</span>.o_proj(out)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> out, attn</span><br></pre></td></tr></table></figure>\r\n<p><strong>注意</strong>：实际工程中常将\r\n<code>q_proj/k_proj/v_proj</code> 分开（尤其在解码器 cross-attn\r\n中），此处为简洁性复用一套权重。</p>\r\n<h2 id=\"残差连接与-layernormpre-ln-vs-post-ln\">3) 残差连接与\r\nLayerNorm（Pre-LN vs Post-LN）</h2>\r\n<p>两种常见放置方式：</p>\r\n<ul>\r\n<li><strong>Post-LN（Vaswani17\r\n原版）</strong>：<code>x = LN(x + Sublayer(x))</code></li>\r\n<li><strong>Pre-LN（更稳定的梯度传播，深层更常用）</strong>：<code>x = x + Sublayer(LN(x))</code></li>\r\n</ul>\r\n<p>本文实现采用 <strong>Pre-LN</strong>。</p>\r\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">ResidualBlock</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, d_model, sublayer, dropout=<span class=\"number\">0.0</span>, pre_norm=<span class=\"literal\">True</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.norm = nn.LayerNorm(d_model)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.sublayer = sublayer</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.dropout = nn.Dropout(dropout)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.pre_norm = pre_norm</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x, *args, **kwargs</span>):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"variable language_\">self</span>.pre_norm:</span><br><span class=\"line\">            y = <span class=\"variable language_\">self</span>.sublayer(<span class=\"variable language_\">self</span>.norm(x), *args, **kwargs)</span><br><span class=\"line\">            y = <span class=\"variable language_\">self</span>.dropout(y)</span><br><span class=\"line\">            <span class=\"keyword\">return</span> x + y</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            y = <span class=\"variable language_\">self</span>.sublayer(x, *args, **kwargs)</span><br><span class=\"line\">            y = <span class=\"variable language_\">self</span>.dropout(y)</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"variable language_\">self</span>.norm(x + y)</span><br></pre></td></tr></table></figure>\r\n<h2 id=\"position-wise-ffn逐位置前馈网络\">4) Position-wise\r\nFFN（逐位置前馈网络）</h2>\r\n<p>论文为 <code>ReLU</code>，许多实现改用 <code>GELU</code>。中间维度\r\n<code>d_ff</code> 通常取 <code>4×d_model</code>。</p>\r\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">PositionwiseFFN</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, d_model, d_ff, dropout=<span class=\"number\">0.0</span>, activation=<span class=\"string\">\"gelu\"</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.fc1 = nn.Linear(d_model, d_ff)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.fc2 = nn.Linear(d_ff, d_model)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.dropout = nn.Dropout(dropout)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.act = nn.GELU() <span class=\"keyword\">if</span> activation.lower()==<span class=\"string\">\"gelu\"</span> <span class=\"keyword\">else</span> nn.ReLU()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"variable language_\">self</span>.fc2(<span class=\"variable language_\">self</span>.dropout(<span class=\"variable language_\">self</span>.act(<span class=\"variable language_\">self</span>.fc1(x))))</span><br></pre></td></tr></table></figure>\r\n<h2 id=\"位置编码positional-encoding\">5) 位置编码（Positional\r\nEncoding）</h2>\r\n<p>固定<strong>正弦/余弦</strong>位置编码（可外推到更长序列）；也可用<strong>可学习</strong>位置嵌入。</p>\r\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> math</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">SinusoidalPositionalEncoding</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, d_model, max_len=<span class=\"number\">10000</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        pe = torch.zeros(max_len, d_model)</span><br><span class=\"line\">        pos = torch.arange(<span class=\"number\">0</span>, max_len, dtype=torch.<span class=\"built_in\">float</span>).unsqueeze(<span class=\"number\">1</span>)</span><br><span class=\"line\">        div = torch.exp(torch.arange(<span class=\"number\">0</span>, d_model, <span class=\"number\">2</span>, dtype=torch.<span class=\"built_in\">float</span>) *</span><br><span class=\"line\">                        -(math.log(<span class=\"number\">10000.0</span>) / d_model))</span><br><span class=\"line\">        pe[:, <span class=\"number\">0</span>::<span class=\"number\">2</span>] = torch.sin(pos * div)</span><br><span class=\"line\">        pe[:, <span class=\"number\">1</span>::<span class=\"number\">2</span>] = torch.cos(pos * div)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.register_buffer(<span class=\"string\">\"pe\"</span>, pe)  <span class=\"comment\"># (max_len, d_model)</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x</span>):</span><br><span class=\"line\">        <span class=\"comment\"># x: (B,S,D)</span></span><br><span class=\"line\">        S = x.size(<span class=\"number\">1</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> x + <span class=\"variable language_\">self</span>.pe[:S].unsqueeze(<span class=\"number\">0</span>)  <span class=\"comment\"># (1,S,D) 广播到 (B,S,D)</span></span><br></pre></td></tr></table></figure>\r\n<blockquote>\r\n<p>现代替代：RoPE、ALiBi 等（不展开）。</p>\r\n</blockquote>\r\n<h2 id=\"编码器层encoder-layer\">6) 编码器层（Encoder Layer）</h2>\r\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">EncoderLayer</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, d_model, num_heads, d_ff, dropout=<span class=\"number\">0.1</span>, pre_norm=<span class=\"literal\">True</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.self_attn = MultiHeadAttention(d_model, num_heads, dropout)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.ffn = PositionwiseFFN(d_model, d_ff, dropout)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.res_attn = ResidualBlock(d_model, <span class=\"variable language_\">self</span>._attn, dropout, pre_norm)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.res_ffn  = ResidualBlock(d_model, <span class=\"variable language_\">self</span>.ffn,  dropout, pre_norm)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">_attn</span>(<span class=\"params\">self, x, attn_mask=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">        y, _ = <span class=\"variable language_\">self</span>.self_attn(x, x, attn_mask=attn_mask)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> y</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x, attn_mask=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">        x = <span class=\"variable language_\">self</span>.res_attn(x, attn_mask=attn_mask)</span><br><span class=\"line\">        x = <span class=\"variable language_\">self</span>.res_ffn(x)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> x</span><br></pre></td></tr></table></figure>\r\n<p><strong>自注意力 mask（padding mask）构造</strong>：</p>\r\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">make_padding_mask</span>(<span class=\"params\">pad_idx, tokens</span>):</span><br><span class=\"line\">    <span class=\"comment\"># tokens: (B,S) 的词 ID，pad 位置为 pad_idx</span></span><br><span class=\"line\">    <span class=\"comment\"># 返回 1 表示可见，0 表示屏蔽</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> (tokens != pad_idx).unsqueeze(<span class=\"number\">1</span>).unsqueeze(<span class=\"number\">1</span>)  <span class=\"comment\"># (B,1,1,S) 广播</span></span><br></pre></td></tr></table></figure>\r\n<h2 id=\"解码器层decoder-layer\">7) 解码器层（Decoder Layer）</h2>\r\n<p>包含：<strong>Masked\r\nSelf-Attn</strong>（因果遮蔽）、<strong>Cross-Attn</strong>（Q 来自\r\ndecoder，K/V 来自 encoder）、<strong>FFN</strong>。</p>\r\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">make_causal_mask</span>(<span class=\"params\">S</span>):</span><br><span class=\"line\">    <span class=\"comment\"># (1,1,S,S) 下三角为 1（可见），上三角为 0（屏蔽）</span></span><br><span class=\"line\">    mask = torch.tril(torch.ones(S, S, dtype=torch.uint8))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> mask.view(<span class=\"number\">1</span>,<span class=\"number\">1</span>,S,S)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">DecoderLayer</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, d_model, num_heads, d_ff, dropout=<span class=\"number\">0.1</span>, pre_norm=<span class=\"literal\">True</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.self_attn = MultiHeadAttention(d_model, num_heads, dropout)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.cross_attn = MultiHeadAttention(d_model, num_heads, dropout)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.ffn = PositionwiseFFN(d_model, d_ff, dropout)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.res_self  = ResidualBlock(d_model, <span class=\"variable language_\">self</span>._self_attn,  dropout, pre_norm)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.res_cross = ResidualBlock(d_model, <span class=\"variable language_\">self</span>._cross_attn, dropout, pre_norm)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.res_ffn   = ResidualBlock(d_model, <span class=\"variable language_\">self</span>.ffn,        dropout, pre_norm)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">_self_attn</span>(<span class=\"params\">self, x, self_mask=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">        y, _ = <span class=\"variable language_\">self</span>.self_attn(x, x, attn_mask=self_mask)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> y</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">_cross_attn</span>(<span class=\"params\">self, x, memory, mem_mask=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">        y, _ = <span class=\"variable language_\">self</span>.cross_attn(x, memory, attn_mask=mem_mask)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> y</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x, memory, self_mask=<span class=\"literal\">None</span>, mem_mask=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">        x = <span class=\"variable language_\">self</span>.res_self(x, self_mask)</span><br><span class=\"line\">        x = <span class=\"variable language_\">self</span>.res_cross(x, memory, mem_mask)</span><br><span class=\"line\">        x = <span class=\"variable language_\">self</span>.res_ffn(x)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> x</span><br></pre></td></tr></table></figure>\r\n<p><strong>注意</strong>：</p>\r\n<ul>\r\n<li>交叉注意力的 <code>attn_mask</code> 常用于 padding 屏蔽（对 encoder\r\nmemory 的 K 维度）；</li>\r\n<li>自注意力要叠加 <strong>因果 mask ∧ padding mask</strong>。</li>\r\n</ul>\r\n<h2 id=\"最小可运行-transformerencoder-decoder\">8) 最小可运行\r\nTransformer（Encoder-Decoder）</h2>\r\n<p>下例仅展示骨架，省略词表、损失与训练循环。</p>\r\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">MiniTransformer</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, vocab_size, d_model=<span class=\"number\">512</span>, num_heads=<span class=\"number\">8</span>, d_ff=<span class=\"number\">2048</span>,</span></span><br><span class=\"line\"><span class=\"params\">                 num_enc_layers=<span class=\"number\">6</span>, num_dec_layers=<span class=\"number\">6</span>, dropout=<span class=\"number\">0.1</span>, pad_idx=<span class=\"number\">0</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.d_model = d_model</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.pad_idx = pad_idx</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.src_embed = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.tgt_embed = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.pos = SinusoidalPositionalEncoding(d_model)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.enc_layers = nn.ModuleList([</span><br><span class=\"line\">            EncoderLayer(d_model, num_heads, d_ff, dropout) <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_enc_layers)</span><br><span class=\"line\">        ])</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.dec_layers = nn.ModuleList([</span><br><span class=\"line\">            DecoderLayer(d_model, num_heads, d_ff, dropout) <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_dec_layers)</span><br><span class=\"line\">        ])</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.lm_head = nn.Linear(d_model, vocab_size)  <span class=\"comment\"># tied weights 可选：与 tgt_embed.weight 共享</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">encode</span>(<span class=\"params\">self, src_tokens</span>):</span><br><span class=\"line\">        x = <span class=\"variable language_\">self</span>.pos(<span class=\"variable language_\">self</span>.src_embed(src_tokens) * math.sqrt(<span class=\"variable language_\">self</span>.d_model))</span><br><span class=\"line\">        src_pad_mask = make_padding_mask(<span class=\"variable language_\">self</span>.pad_idx, src_tokens)  <span class=\"comment\"># (B,1,1,S_src)</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> layer <span class=\"keyword\">in</span> <span class=\"variable language_\">self</span>.enc_layers:</span><br><span class=\"line\">            x = layer(x, attn_mask=src_pad_mask)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> x, src_pad_mask</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">decode</span>(<span class=\"params\">self, tgt_tokens, memory, src_pad_mask</span>):</span><br><span class=\"line\">        x = <span class=\"variable language_\">self</span>.pos(<span class=\"variable language_\">self</span>.tgt_embed(tgt_tokens) * math.sqrt(<span class=\"variable language_\">self</span>.d_model))</span><br><span class=\"line\">        B, S_t = tgt_tokens.shape</span><br><span class=\"line\">        causal = make_causal_mask(S_t).to(tgt_tokens.device)       <span class=\"comment\"># (1,1,S_t,S_t)</span></span><br><span class=\"line\">        tgt_pad_mask = make_padding_mask(<span class=\"variable language_\">self</span>.pad_idx, tgt_tokens) <span class=\"comment\"># (B,1,1,S_t)</span></span><br><span class=\"line\">        self_mask = causal &amp; tgt_pad_mask                          <span class=\"comment\"># 逻辑与</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> layer <span class=\"keyword\">in</span> <span class=\"variable language_\">self</span>.dec_layers:</span><br><span class=\"line\">            x = layer(x, memory, self_mask, src_pad_mask)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> x</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, src_tokens, tgt_tokens</span>):</span><br><span class=\"line\">        memory, src_pad_mask = <span class=\"variable language_\">self</span>.encode(src_tokens)</span><br><span class=\"line\">        dec_out = <span class=\"variable language_\">self</span>.decode(tgt_tokens, memory, src_pad_mask)</span><br><span class=\"line\">        logits = <span class=\"variable language_\">self</span>.lm_head(dec_out)  <span class=\"comment\"># (B,S_t,V)</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> logits</span><br></pre></td></tr></table></figure>\r\n<p><strong>贪心生成（示例）</strong>：</p>\r\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@torch.no_grad()</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">greedy_decode</span>(<span class=\"params\">model, src_tokens, bos_id, eos_id, max_len=<span class=\"number\">128</span></span>):</span><br><span class=\"line\">    device = src_tokens.device</span><br><span class=\"line\">    memory, src_pad_mask = model.encode(src_tokens)</span><br><span class=\"line\">    B = src_tokens.size(<span class=\"number\">0</span>)</span><br><span class=\"line\">    ys = torch.full((B,<span class=\"number\">1</span>), bos_id, dtype=torch.long, device=device)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(max_len-<span class=\"number\">1</span>):</span><br><span class=\"line\">        dec_out = model.decode(ys, memory, src_pad_mask)   <span class=\"comment\"># (B,S,D)</span></span><br><span class=\"line\">        next_logit = model.lm_head(dec_out[:, -<span class=\"number\">1</span>, :])      <span class=\"comment\"># (B,V)</span></span><br><span class=\"line\">        next_token = next_logit.argmax(-<span class=\"number\">1</span>, keepdim=<span class=\"literal\">True</span>)   <span class=\"comment\"># (B,1)</span></span><br><span class=\"line\">        ys = torch.cat([ys, next_token], dim=<span class=\"number\">1</span>)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (next_token == eos_id).<span class=\"built_in\">all</span>(): <span class=\"keyword\">break</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> ys</span><br></pre></td></tr></table></figure>\r\n<h2 id=\"复杂度与工程优化\">9) 复杂度与工程优化</h2>\r\n<ul>\r\n<li>注意力复杂度 <code>O(S^2·D)</code>，内存\r\n<code>O(S^2)</code>；长序列瓶颈明显；</li>\r\n<li>典型优化：FlashAttention、块稀疏/滑窗注意力、低秩核近似、KV Cache\r\n等；</li>\r\n<li>训练 trick：Label Smoothing、学习率\r\nwarmup、梯度裁剪、Dropout/Attention Dropout、权重共享（tied\r\nembeddings）。</li>\r\n</ul>\r\n<h2 id=\"常见坑位清单\">10) 常见坑位清单</h2>\r\n<ol type=\"1\">\r\n<li><strong>mask 方向/形状</strong>：应能广播到\r\n<code>(B,h,S_q,S_k)</code>；注意 batch-first；<br>\r\n</li>\r\n<li><strong>dtype</strong>：mask 多用\r\n<code>bool</code>/<code>uint8</code>；被屏蔽处加 <code>-inf</code>\r\n前需确保 dtype 是浮点；<br>\r\n</li>\r\n<li><strong>全屏蔽→NaN</strong>：确保每个查询位置至少有一个可见键；<br>\r\n</li>\r\n<li><strong>缩放因子</strong>：别忘了 <span class=\"math inline\"><mjx-container class=\"MathJax\" jax=\"SVG\"><svg style=\"vertical-align: -0.566ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"5.259ex\" height=\"2.592ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -895.6 2324.4 1145.6\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"TeXAtom\" data-mjx-texclass=\"ORD\"><g data-mml-node=\"mo\"><path data-c=\"2F\" d=\"M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z\"></path></g></g><g data-mml-node=\"msqrt\" transform=\"translate(500,0)\"><g transform=\"translate(853,0)\"><g data-mml-node=\"msub\"><g data-mml-node=\"mi\"><path data-c=\"1D451\" d=\"M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(553,-150) scale(0.707)\"><path data-c=\"1D458\" d=\"M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z\"></path></g></g></g><g data-mml-node=\"mo\" transform=\"translate(0,35.6)\"><path data-c=\"221A\" d=\"M95 178Q89 178 81 186T72 200T103 230T169 280T207 309Q209 311 212 311H213Q219 311 227 294T281 177Q300 134 312 108L397 -77Q398 -77 501 136T707 565T814 786Q820 800 834 800Q841 800 846 794T853 782V776L620 293L385 -193Q381 -200 366 -200Q357 -200 354 -197Q352 -195 256 15L160 225L144 214Q129 202 113 190T95 178Z\"></path></g><rect width=\"971.4\" height=\"60\" x=\"853\" y=\"775.6\"></rect></g></g></g></svg></mjx-container></span>；<br>\r\n</li>\r\n<li><strong>Pre-LN / Post-LN 混用</strong>：训练不稳定时优先用\r\nPre-LN；<br>\r\n</li>\r\n<li><strong>位置编码尺度</strong>：将嵌入乘 <code>sqrt(d_model)</code>\r\n再相加位置编码，数值更稳定；<br>\r\n</li>\r\n<li><strong>batch-first 与官方 API</strong>：<code>nn.Transformer</code>\r\n默认 seq-first，注意对齐；<br>\r\n</li>\r\n<li><strong>KV Cache</strong>（推理）：解码增量生成要缓存 past\r\nK/V，避免二次方重复计算。</li>\r\n</ol>\r\n<h2 id=\"参考实现最小依赖清单\">11) 参考实现最小依赖清单</h2>\r\n<ul>\r\n<li>Python ≥ 3.8<br>\r\n</li>\r\n<li>PyTorch ≥ 1.12（支持 batch-first 层归一化与常规算子即可）</li>\r\n</ul>\r\n<h2 id=\"致谢与参考\">12) 致谢与参考</h2>\r\n<ul>\r\n<li>Vaswani et al., <em>Attention Is All You Need</em>, NeurIPS\r\n2017.<br>\r\n</li>\r\n<li>PyTorch 文档：<code>nn.MultiheadAttention</code>,\r\n<code>nn.Transformer</code>。<br>\r\n</li>\r\n<li>相关工程优化可参考：FlashAttention、RoPE/ALiBi/相对位置编码等论文与实现。</li>\r\n</ul>\r\n","feature":true,"text":"读者对象：已具备基本深度学习与 PyTorch 基础，希望系统掌握 Transformer 各模块设计与实现的工程师/学生。 文章目标：从实现角度深入讲清楚每个...","permalink":"/post/transformer","photos":[],"count_time":{"symbolsCount":"11k","symbolsTime":"10 mins."},"categories":[{"name":"人工智能","slug":"人工智能","count":3,"path":"api/categories/人工智能.json"}],"tags":[{"name":"transformer","slug":"transformer","count":1,"path":"api/tags/transformer.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%80%BB%E8%A7%88%E5%BD%A2%E7%8A%B6%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%B5%81\"><span class=\"toc-text\">总览（形状与数据流）</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#scaled-dot-product-attention%E7%BC%A9%E6%94%BE%E7%82%B9%E7%A7%AF%E6%B3%A8%E6%84%8F%E5%8A%9B\"><span class=\"toc-text\">1) Scaled\r\nDot-Product Attention（缩放点积注意力）</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#multi-head-attention%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B\"><span class=\"toc-text\">2) Multi-Head\r\nAttention（多头注意力）</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5%E4%B8%8E-layernormpre-ln-vs-post-ln\"><span class=\"toc-text\">3) 残差连接与\r\nLayerNorm（Pre-LN vs Post-LN）</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#position-wise-ffn%E9%80%90%E4%BD%8D%E7%BD%AE%E5%89%8D%E9%A6%88%E7%BD%91%E7%BB%9C\"><span class=\"toc-text\">4) Position-wise\r\nFFN（逐位置前馈网络）</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81positional-encoding\"><span class=\"toc-text\">5) 位置编码（Positional\r\nEncoding）</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E7%BC%96%E7%A0%81%E5%99%A8%E5%B1%82encoder-layer\"><span class=\"toc-text\">6) 编码器层（Encoder Layer）</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E8%A7%A3%E7%A0%81%E5%99%A8%E5%B1%82decoder-layer\"><span class=\"toc-text\">7) 解码器层（Decoder Layer）</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%9C%80%E5%B0%8F%E5%8F%AF%E8%BF%90%E8%A1%8C-transformerencoder-decoder\"><span class=\"toc-text\">8) 最小可运行\r\nTransformer（Encoder-Decoder）</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E5%A4%8D%E6%9D%82%E5%BA%A6%E4%B8%8E%E5%B7%A5%E7%A8%8B%E4%BC%98%E5%8C%96\"><span class=\"toc-text\">9) 复杂度与工程优化</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E5%B8%B8%E8%A7%81%E5%9D%91%E4%BD%8D%E6%B8%85%E5%8D%95\"><span class=\"toc-text\">10) 常见坑位清单</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E5%8F%82%E8%80%83%E5%AE%9E%E7%8E%B0%E6%9C%80%E5%B0%8F%E4%BE%9D%E8%B5%96%E6%B8%85%E5%8D%95\"><span class=\"toc-text\">11) 参考实现最小依赖清单</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E8%87%B4%E8%B0%A2%E4%B8%8E%E5%8F%82%E8%80%83\"><span class=\"toc-text\">12) 致谢与参考</span></a></li></ol>","author":{"name":"Rockway","slug":"blog-author","avatar":"https://lingmafuture.github.io/images/3.jpg","link":"https://lingmafuture.github.io","description":"一个充满情怀的AI技术探索者，欢迎交流人工智能、量化交易、科技产品、创业灵感。","socials":{"github":"https://github.com/lingmafuture","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"mapped":true,"hidden":false,"prev_post":{"title":"Python常用数据处理库概览","uid":"d18fcd6dc1394e0e934c5a6ec63149f0","slug":"python-libraries","date":"2025-08-14T16:00:00.000Z","updated":"2025-08-29T11:32:02.947Z","comments":true,"path":"api/articles/python-libraries.json","keywords":"blog, technology, programming","cover":"images/python-libraries.png","text":"引言 在数据科学、数据工程和数据分析领域，数据处理是必不可少的基础环节。业界常说数据科学家将大部分时间花在整理清洗数据上，有经验的分析师都深知这一点：整个数据分...","permalink":"/post/python-libraries","photos":[],"count_time":{"symbolsCount":"12k","symbolsTime":"11 mins."},"categories":[{"name":"算法","slug":"算法","count":2,"path":"api/categories/算法.json"}],"tags":[{"name":"python","slug":"python","count":2,"path":"api/tags/python.json"},{"name":"数据处理","slug":"数据处理","count":1,"path":"api/tags/数据处理.json"}],"author":{"name":"Rockway","slug":"blog-author","avatar":"https://lingmafuture.github.io/images/3.jpg","link":"https://lingmafuture.github.io","description":"一个充满情怀的AI技术探索者，欢迎交流人工智能、量化交易、科技产品、创业灵感。","socials":{"github":"https://github.com/lingmafuture","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"feature":true},"next_post":{"title":"Hexo 博客搭建指南：Aurora 主题与 Cloudflare + GitHub Pages 部署","uid":"7c571cda647bd463004b1d6057d46f3c","slug":"hexo-guide","date":"2025-07-17T16:00:00.000Z","updated":"2025-08-19T13:07:10.419Z","comments":true,"path":"api/articles/hexo-guide.json","keywords":"blog, technology, programming","cover":"images/hexo-guide.png","text":"Hexo 是一个基于 Node.js 的快速、高效的静态博客框架。通过 Hexo，我们可以使用 Markdown 编写文章，几秒钟内生成静态网页并部署到托管服务...","permalink":"/post/hexo-guide","photos":[],"count_time":{"symbolsCount":"18k","symbolsTime":"17 mins."},"categories":[{"name":"建站","slug":"建站","count":2,"path":"api/categories/建站.json"}],"tags":[{"name":"hexo","slug":"hexo","count":2,"path":"api/tags/hexo.json"},{"name":"教程","slug":"教程","count":2,"path":"api/tags/教程.json"}],"author":{"name":"Rockway","slug":"blog-author","avatar":"https://lingmafuture.github.io/images/3.jpg","link":"https://lingmafuture.github.io","description":"一个充满情怀的AI技术探索者，欢迎交流人工智能、量化交易、科技产品、创业灵感。","socials":{"github":"https://github.com/lingmafuture","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"feature":true}}