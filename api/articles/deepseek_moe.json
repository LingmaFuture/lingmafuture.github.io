{"title":"DeepSeek 模型原理解析（重点：MoE 架构）","uid":"9b46d1c5a75e2d41735eee2edeede3f3","slug":"deepseek_moe","date":"2025-08-22T16:00:00.000Z","updated":"2025-08-23T01:54:16.639Z","comments":true,"path":"api/articles/deepseek_moe.json","keywords":"blog, technology, programming","cover":"images/deepseek_moe.png","content":"<p>面向具有一定深度学习基础的读者，本文系统解析\r\n<strong>DeepSeek</strong> 系列模型的原理与设计特点，重点关注其\r\n<strong>Mixture-of-Experts (MoE)</strong>\r\n稀疏架构、关键实现、训练策略与性能取舍。文末给出发展展望。为便于分发与二次创作，全文为\r\nMarkdown 格式且不含外链。</p>\r\n<hr />\r\n<h2 id=\"整体设计与定位\">1. 整体设计与定位</h2>\r\n<p>DeepSeek\r\n的目标是在<strong>有限算力与成本约束</strong>下达到与顶级闭源模型相当的综合能力，同时以<strong>开源</strong>的形式降低使用与定制门槛。核心思路：\r\n- 以 <strong>MoE 稀疏专家</strong>替代传统 Transformer\r\n中的密集前馈网络（FFN），显著提升<strong>参数容量/计算开销</strong>的性价比。\r\n-\r\n在推理阶段仅激活少量专家，使得“有效参与计算的参数量”远小于“总参数量”，在<strong>保持模型容量</strong>的同时降低<strong>延迟与成本</strong>。\r\n-\r\n通过工程化与训练策略（通信重叠、混合精度、蒸馏与对齐等）在实际集群上稳定训练超大规模稀疏模型。</p>\r\n<p>DeepSeek 家族常见特征： - 总参数可达数千亿级，但<strong>每 token\r\n激活</strong>仅约 <strong>数十亿到数百亿</strong>参数。 -\r\n强调<strong>中文、代码、数学推理</strong>等场景的实用性与性价比。</p>\r\n<hr />\r\n<h2 id=\"moe-架构剖析\">2. MoE 架构剖析</h2>\r\n<h3 id=\"稀疏激活sparse-activation\">2.1 稀疏激活（Sparse\r\nActivation）</h3>\r\n<p>在传统密集模型里，每个 token 都会计算同一套巨大的 FFN；MoE 将 FFN\r\n替换为<strong>多个专家</strong>（experts）并由<strong>路由器</strong>（router/gating\r\nnetwork）根据输入内容选择<strong>Top‑K</strong>个最相关专家参与计算。以常见配置为例：\r\n- 每层包含 <strong>256</strong> 个专家（含 1\r\n个<strong>共享专家</strong>）； - 路由器为每个 token 选出\r\n<strong>K=8</strong> 个专家，再<strong>加上共享专家</strong>一并参与（共\r\n9 个）； - 整体模型总参数可达 <strong>≈6,710 亿</strong>，但<strong>每个\r\ntoken 实际激活约 370 亿参数</strong>参与一次前向。</p>\r\n<blockquote>\r\n<p>结果：在不牺牲容量的情形下，<strong>推理计算量</strong>近似等效于中型密集模型，<strong>吞吐与时延</strong>显著优化。</p>\r\n</blockquote>\r\n<h3 id=\"专家路由与容量约束\">2.2 专家路由与容量约束</h3>\r\n<p>路由器（可看作一个小型网络）对每个 token\r\n计算到各专家的<strong>亲和分数</strong>，选 Top‑K\r\n专家。为避免<strong>负载倾斜</strong>： -\r\n为每位专家设置<strong>容量上限</strong>（一次最多接收多少\r\ntokens），超过上限的 token 将被路由到次优专家； -\r\n训练早期可引入<strong>随机抖动/温度</strong>，提升探索性，减少“路由塌陷”（总挑固定少数专家）。</p>\r\n<h3 id=\"无辅助损失的负载均衡\">2.3 无辅助损失的负载均衡</h3>\r\n<p>传统 MoE 常在损失中加入均衡正则（例如 Switch 系列的 load‑balancing\r\nloss）。DeepSeek 实践了一种<strong>无显式辅助损失</strong>的均衡法： -\r\n给专家引入<strong>可训练偏置</strong>（bias to routing\r\nscore），让使用率偏低的专家获得更高被选概率； -\r\n在不干扰主任务损失的前提下，<strong>自适应提升</strong>专家利用率的均衡性与稳定性；\r\n-\r\n可辅以<strong>序列级的轻量约束</strong>，避免单个序列内出现极端不均衡。</p>\r\n<h3 id=\"共享专家与知识隔离\">2.4 共享专家与知识隔离</h3>\r\n<p>设置一个<strong>共享专家（Shared Expert）</strong>在每个 token\r\n上始终激活： -\r\n将<strong>常识性/通用规律</strong>收敛在共享专家，减少在多个专家之间的知识重复；\r\n-\r\n让其余专家专注于<strong>领域特化</strong>（如代码、数学、检索式问答等）；\r\n- 实践表明可增强<strong>总体稳定性</strong>与<strong>泛化</strong>。</p>\r\n<h3 id=\"专家特化与可扩展性\">2.5 专家特化与可扩展性</h3>\r\n<p>MoE 的“模块化”带来两点收益： -\r\n<strong>特化</strong>：各专家围绕不同能力分工（代码/数学/常识/工具调用等），组合后能力多样；\r\n-\r\n<strong>可扩展</strong>：新增/替换专家几乎<strong>不改变</strong>单次推理计算量，便于<strong>按需扩容</strong>与<strong>领域定制</strong>。</p>\r\n<h3 id=\"结构对比与示意mermaid\">2.6 结构对比与示意（Mermaid）</h3>\r\n<h4 id=\"密集-ffn\">2.6.1 密集 FFN</h4>\r\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">flowchart LR</span><br><span class=\"line\">  A[Token 表示] --&gt; B[自注意力层]</span><br><span class=\"line\">  B --&gt; C[密集 FFN (全参与)]</span><br><span class=\"line\">  C --&gt; D[输出]</span><br></pre></td></tr></table></figure>\r\n<h4 id=\"稀疏-moetopk-shared-expert\">2.6.2 稀疏 MoE（Top‑K + Shared\r\nExpert）</h4>\r\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">flowchart LR</span><br><span class=\"line\">  A[Token 表示] --&gt; B[自注意力层]</span><br><span class=\"line\">  B --&gt; R[路由器 Router]</span><br><span class=\"line\">  subgraph MoE Block</span><br><span class=\"line\">    R --&gt; E1[共享专家 Shared]</span><br><span class=\"line\">    R --&gt; E2[专家 1]</span><br><span class=\"line\">    R --&gt; E3[专家 2]</span><br><span class=\"line\">    R --&gt; E4[专家 ...]</span><br><span class=\"line\">    R --&gt; Ek[专家 K]</span><br><span class=\"line\">    E1 --&gt; M[专家输出聚合/加权求和]</span><br><span class=\"line\">    E2 --&gt; M</span><br><span class=\"line\">    E3 --&gt; M</span><br><span class=\"line\">    E4 --&gt; M</span><br><span class=\"line\">    Ek --&gt; M</span><br><span class=\"line\">  end</span><br><span class=\"line\">  M --&gt; D[输出]</span><br></pre></td></tr></table></figure>\r\n<blockquote>\r\n<p>对比要点：密集 FFN 对所有 token <strong>全参与</strong>；MoE 通过\r\n<strong>Router</strong>\r\n仅激活<strong>少量专家+共享专家</strong>，再对结果聚合。</p>\r\n</blockquote>\r\n<hr />\r\n<h2 id=\"训练策略与实现细节\">3. 训练策略与实现细节</h2>\r\n<h3 id=\"海量预训练与工程化\">3.1 海量预训练与工程化</h3>\r\n<ul>\r\n<li><strong>数据规模</strong>：多语言为主，覆盖自然语言、代码、数学与科学文本；token\r\n规模可达 <strong>≈14.8T</strong> 量级；</li>\r\n<li><strong>混合精度</strong>：在超大规模上实测 <strong>FP8</strong>（与\r\nFP16/BF16 混用）可保持稳定收敛并显著提高吞吐；</li>\r\n<li><strong>分布式优化</strong>：深度流水并行、张量并行、专家并行，<strong>通信与计算重叠</strong>，显著降低“稀疏模型\r\n= 通信密集”的开销；</li>\r\n<li><strong>算耗量级</strong>：完整预训练约 <strong>百万级\r\nGPU·小时</strong>（如基于 H800/H100\r\n集群），损失曲线稳定，无需大规模回滚。</li>\r\n</ul>\r\n<h3 id=\"多阶段微调sft-蒸馏-rl-对齐\">3.2 多阶段微调：SFT → 蒸馏 → RL\r\n对齐</h3>\r\n<ul>\r\n<li><strong>SFT（监督微调）</strong>：指令遵循、对话与工具使用示例，强化可控性与易用性；</li>\r\n<li><strong>从强推理模型蒸馏</strong>：将“长链思维/过程推理”示例蒸馏到通用模型，使其<strong>保留强推理</strong>同时<strong>输出更简洁</strong>；</li>\r\n<li><strong>RLHF / Group‑RPO 等</strong>：结合规则 +\r\n模型奖励，优化<strong>有用性、真实性、安全性</strong>，抑制幻觉与不当输出。</li>\r\n</ul>\r\n<h3 id=\"训练目标改造多-token-并行预测mtp\">3.3 训练目标改造：多 Token\r\n并行预测（MTP）</h3>\r\n<ul>\r\n<li>在语言建模目标上引入 <strong>Multi‑Token\r\nPrediction</strong>：一次性预测后续多个 token；</li>\r\n<li>经验上可增强<strong>长依赖建模</strong>与<strong>推理连贯性</strong>；</li>\r\n<li>与<strong>推测解码（Speculative\r\nDecoding）</strong>天然兼容，小模型生成候选、大模型并行校验，加速推理。</li>\r\n</ul>\r\n<h3 id=\"长上下文\">3.4 长上下文</h3>\r\n<ul>\r\n<li>支持<strong>超长上下文</strong>（典型可达 <strong>128K</strong>\r\ntokens 量级）；</li>\r\n<li>结合相对位置编码、多查询注意力/多头布局优化等手段，兼顾<strong>上下文长度</strong>与<strong>吞吐</strong>。</li>\r\n</ul>\r\n<hr />\r\n<h2 id=\"性能与取舍\">4. 性能与取舍</h2>\r\n<h3 id=\"能力画像\">4.1 能力画像</h3>\r\n<ul>\r\n<li><strong>强项</strong>：代码生成、数学推理、中文任务（阅读理解、百科问答、写作）等；</li>\r\n<li><strong>短板</strong>：部分英文常识问答与事实性检索题可能略逊于同级别顶尖密集模型；</li>\r\n<li><strong>原因</strong>（可能）：路由导致知识分散、语料侧重差异、对齐偏好不同等。</li>\r\n</ul>\r\n<h3 id=\"推理速度与成本\">4.2 推理速度与成本</h3>\r\n<ul>\r\n<li><strong>每 token 仅激活少量专家</strong>（如 8 个 +\r\n共享专家），<strong>有效参数量 ≪ 总参数量</strong>；</li>\r\n<li>在相同硬件下，吞吐与时延<strong>优于</strong>同规模密集模型，<strong>单位成本显著降低</strong>；</li>\r\n<li>适合<strong>大规模在线服务</strong>与<strong>本地化部署</strong>的场景，具备极高<strong>性价比</strong>。</li>\r\n</ul>\r\n<h3 id=\"训练效率\">4.3 训练效率</h3>\r\n<ul>\r\n<li>相比“同等效果”的密集模型，MoE\r\n训练<strong>算耗可降到量级更低</strong>（经验上可达<strong>约\r\n1/10</strong>）；</li>\r\n<li>关键在于：通信优化 + 容量约束 + 负载均衡 +\r\n稳定路由，使稀疏训练可控、可复现。</li>\r\n</ul>\r\n<hr />\r\n<h2 id=\"研发陷阱与工程要点实践向\">5. 研发陷阱与工程要点（实践向）</h2>\r\n<ul>\r\n<li><strong>路由塌陷</strong>：早期加入温度与随机性；使用容量上限与可训练偏置提升均衡性。<br />\r\n</li>\r\n<li><strong>梯度不稳/爆炸</strong>：梯度裁剪、优化器学习率热身、损失缩放（AMP）、稳定化激活（如\r\nSwiGLU）。<br />\r\n</li>\r\n<li><strong>通信瓶颈</strong>：专家并行 + token\r\n重排/聚合；流水并行分层；尽量<strong>计算‑通信重叠</strong>。<br />\r\n</li>\r\n<li><strong>内存开销</strong>：总权重需要常驻（参数并行/ZeRO），推理可结合<strong>专家裁剪/蒸馏</strong>降低显存；离线量化（INT8/INT4）在部署侧非常有效。<br />\r\n</li>\r\n<li><strong>评测与对齐</strong>：区分<strong>推理能力</strong>与<strong>事实性</strong>；中文/代码/数学单列评估，英文常识补强检索/RAG。</li>\r\n</ul>\r\n<hr />\r\n<h2 id=\"总结与展望\">6. 总结与展望</h2>\r\n<ul>\r\n<li><strong>总结</strong>：DeepSeek 通过 <strong>MoE\r\n稀疏专家</strong>实现了“<strong>大容量、低计算</strong>”的良性组合；配合\r\n<strong>FP8 + 并行与通信优化 + 蒸馏 + RL 对齐 +\r\nMTP</strong>，在<strong>中文、代码、数学</strong>等方向形成<strong>高性价比</strong>优势。<br />\r\n</li>\r\n<li><strong>趋势</strong>：\r\n<ul>\r\n<li><strong>思考模式切换</strong>（快答\r\nvs. 逐步推理）以提升复杂任务效率；</li>\r\n<li><strong>Agent 化与工具使用</strong>增强；</li>\r\n<li><strong>动态专家</strong>与<strong>更智能的路由</strong>、多模态\r\nMoE、跨任务/跨域专家共享。<br />\r\n</li>\r\n</ul></li>\r\n<li><strong>对开发者的建议</strong>：\r\n<ol type=\"1\">\r\n<li>小规模上先验证\r\n<strong>Top‑K、容量、负载均衡</strong>超参对稳定性的影响；<br />\r\n</li>\r\n<li>将<strong>领域数据</strong>做成“专家‑友好”的微调/蒸馏集，促成<strong>专家特化</strong>；<br />\r\n</li>\r\n<li>部署侧优先做<strong>量化 + KV Cache 优化 +\r\n推测解码</strong>，在保证质量的前提下最大化吞吐。</li>\r\n</ol></li>\r\n</ul>\r\n<hr />\r\n<h3 id=\"附录一页速览cheatsheet\">附录：一页速览（Cheatsheet）</h3>\r\n<ul>\r\n<li><strong>MoE = 大容量 + 稀疏计算</strong>；Top‑K + Shared Expert\r\n是高性价比默认配置。<br />\r\n</li>\r\n<li><strong>稳定训练关键</strong>：容量约束、可训练偏置均衡、通信重叠、FP8、梯度裁剪。<br />\r\n</li>\r\n<li><strong>能力结构</strong>：中文/代码/数学强，英文常识可用\r\nRAG/检索补齐。<br />\r\n</li>\r\n<li><strong>部署优先级</strong>：量化 → KV 缓存 → 推测解码 →\r\n并行流水化。</li>\r\n</ul>\r\n","feature":true,"text":"面向具有一定深度学习基础的读者，本文系统解析 DeepSeek 系列模型的原理与设计特点，重点关注其 Mixture-of-Experts (MoE) 稀疏架构...","permalink":"/post/deepseek_moe","photos":[],"count_time":{"symbolsCount":"3.9k","symbolsTime":"4 mins."},"categories":[{"name":"人工智能","slug":"人工智能","count":3,"path":"api/categories/人工智能.json"}],"tags":[{"name":"deepseek","slug":"deepseek","count":1,"path":"api/tags/deepseek.json"},{"name":"MoE","slug":"MoE","count":1,"path":"api/tags/MoE.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%95%B4%E4%BD%93%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9A%E4%BD%8D\"><span class=\"toc-text\">1. 整体设计与定位</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#moe-%E6%9E%B6%E6%9E%84%E5%89%96%E6%9E%90\"><span class=\"toc-text\">2. MoE 架构剖析</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E7%A8%80%E7%96%8F%E6%BF%80%E6%B4%BBsparse-activation\"><span class=\"toc-text\">2.1 稀疏激活（Sparse\r\nActivation）</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E4%B8%93%E5%AE%B6%E8%B7%AF%E7%94%B1%E4%B8%8E%E5%AE%B9%E9%87%8F%E7%BA%A6%E6%9D%9F\"><span class=\"toc-text\">2.2 专家路由与容量约束</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E6%97%A0%E8%BE%85%E5%8A%A9%E6%8D%9F%E5%A4%B1%E7%9A%84%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1\"><span class=\"toc-text\">2.3 无辅助损失的负载均衡</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%85%B1%E4%BA%AB%E4%B8%93%E5%AE%B6%E4%B8%8E%E7%9F%A5%E8%AF%86%E9%9A%94%E7%A6%BB\"><span class=\"toc-text\">2.4 共享专家与知识隔离</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E4%B8%93%E5%AE%B6%E7%89%B9%E5%8C%96%E4%B8%8E%E5%8F%AF%E6%89%A9%E5%B1%95%E6%80%A7\"><span class=\"toc-text\">2.5 专家特化与可扩展性</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E7%BB%93%E6%9E%84%E5%AF%B9%E6%AF%94%E4%B8%8E%E7%A4%BA%E6%84%8Fmermaid\"><span class=\"toc-text\">2.6 结构对比与示意（Mermaid）</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E5%AF%86%E9%9B%86-ffn\"><span class=\"toc-text\">2.6.1 密集 FFN</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E7%A8%80%E7%96%8F-moetopk-shared-expert\"><span class=\"toc-text\">2.6.2 稀疏 MoE（Top‑K + Shared\r\nExpert）</span></a></li></ol></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E8%AE%AD%E7%BB%83%E7%AD%96%E7%95%A5%E4%B8%8E%E5%AE%9E%E7%8E%B0%E7%BB%86%E8%8A%82\"><span class=\"toc-text\">3. 训练策略与实现细节</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E6%B5%B7%E9%87%8F%E9%A2%84%E8%AE%AD%E7%BB%83%E4%B8%8E%E5%B7%A5%E7%A8%8B%E5%8C%96\"><span class=\"toc-text\">3.1 海量预训练与工程化</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%A4%9A%E9%98%B6%E6%AE%B5%E5%BE%AE%E8%B0%83sft-%E8%92%B8%E9%A6%8F-rl-%E5%AF%B9%E9%BD%90\"><span class=\"toc-text\">3.2 多阶段微调：SFT → 蒸馏 → RL\r\n对齐</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E8%AE%AD%E7%BB%83%E7%9B%AE%E6%A0%87%E6%94%B9%E9%80%A0%E5%A4%9A-token-%E5%B9%B6%E8%A1%8C%E9%A2%84%E6%B5%8Bmtp\"><span class=\"toc-text\">3.3 训练目标改造：多 Token\r\n并行预测（MTP）</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87\"><span class=\"toc-text\">3.4 长上下文</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%80%A7%E8%83%BD%E4%B8%8E%E5%8F%96%E8%88%8D\"><span class=\"toc-text\">4. 性能与取舍</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E8%83%BD%E5%8A%9B%E7%94%BB%E5%83%8F\"><span class=\"toc-text\">4.1 能力画像</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6%E4%B8%8E%E6%88%90%E6%9C%AC\"><span class=\"toc-text\">4.2 推理速度与成本</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E8%AE%AD%E7%BB%83%E6%95%88%E7%8E%87\"><span class=\"toc-text\">4.3 训练效率</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E7%A0%94%E5%8F%91%E9%99%B7%E9%98%B1%E4%B8%8E%E5%B7%A5%E7%A8%8B%E8%A6%81%E7%82%B9%E5%AE%9E%E8%B7%B5%E5%90%91\"><span class=\"toc-text\">5. 研发陷阱与工程要点（实践向）</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%80%BB%E7%BB%93%E4%B8%8E%E5%B1%95%E6%9C%9B\"><span class=\"toc-text\">6. 总结与展望</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E9%99%84%E5%BD%95%E4%B8%80%E9%A1%B5%E9%80%9F%E8%A7%88cheatsheet\"><span class=\"toc-text\">附录：一页速览（Cheatsheet）</span></a></li></ol></li></ol>","author":{"name":"Rockway","slug":"blog-author","avatar":"https://lingmafuture.github.io/images/3.jpg","link":"https://lingmafuture.github.io","description":"一个充满情怀的AI技术探索者，欢迎交流人工智能、大模型、深度学习等技术。","socials":{"github":"https://github.com/lingmafuture","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"mapped":true,"hidden":false,"prev_post":{},"next_post":{"title":"多模态行人 ReID 全量微调的过拟合问题与解决方案","uid":"32fbe71fdaa98cf4cb13d70c56ad0143","slug":"multimodal_reid","date":"2025-08-19T16:00:00.000Z","updated":"2025-08-19T13:21:36.221Z","comments":true,"path":"api/articles/multimodal_reid.json","keywords":"blog, technology, programming","cover":"images/multimodal_reid.png","text":"多模态行人重识别（ReID）旨在利用多种数据模态（如可见光图像、红外图像、素描、彩色手绘图、文本描述等）来匹配行人身份。在最新的 ORBench 数据集中，每个...","permalink":"/post/multimodal_reid","photos":[],"count_time":{"symbolsCount":"10k","symbolsTime":"9 mins."},"categories":[{"name":"人工智能","slug":"人工智能","count":3,"path":"api/categories/人工智能.json"}],"tags":[{"name":"多模态","slug":"多模态","count":1,"path":"api/tags/多模态.json"},{"name":"CV","slug":"CV","count":1,"path":"api/tags/CV.json"}],"author":{"name":"Rockway","slug":"blog-author","avatar":"https://lingmafuture.github.io/images/3.jpg","link":"https://lingmafuture.github.io","description":"一个充满情怀的AI技术探索者，欢迎交流人工智能、大模型、深度学习等技术。","socials":{"github":"https://github.com/lingmafuture","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"feature":true}}