{"title":"DeepSeek 模型原理解析（重点：MoE 架构）","uid":"9b46d1c5a75e2d41735eee2edeede3f3","slug":"deepseek_moe","date":"2025-08-22T16:00:00.000Z","updated":"2025-08-23T01:54:16.639Z","comments":true,"path":"api/articles/deepseek_moe.json","keywords":"blog, technology, programming","cover":"images/deepseek_moe.png","content":"<p>面向具有一定深度学习基础的读者，本文系统解析 <strong>DeepSeek</strong> 系列模型的原理与设计特点，重点关注其 <strong>Mixture-of-Experts (MoE)</strong> 稀疏架构、关键实现、训练策略与性能取舍。文末给出发展展望。为便于分发与二次创作，全文为 Markdown 格式且不含外链。</p>\n<hr>\n<h2 id=\"1-整体设计与定位\"><a href=\"#1-整体设计与定位\" class=\"headerlink\" title=\"1. 整体设计与定位\"></a>1. 整体设计与定位</h2><p>DeepSeek 的目标是在<strong>有限算力与成本约束</strong>下达到与顶级闭源模型相当的综合能力，同时以<strong>开源</strong>的形式降低使用与定制门槛。核心思路：</p>\n<ul>\n<li>以 <strong>MoE 稀疏专家</strong>替代传统 Transformer 中的密集前馈网络（FFN），显著提升<strong>参数容量/计算开销</strong>的性价比。</li>\n<li>在推理阶段仅激活少量专家，使得“有效参与计算的参数量”远小于“总参数量”，在<strong>保持模型容量</strong>的同时降低<strong>延迟与成本</strong>。</li>\n<li>通过工程化与训练策略（通信重叠、混合精度、蒸馏与对齐等）在实际集群上稳定训练超大规模稀疏模型。</li>\n</ul>\n<p>DeepSeek 家族常见特征：</p>\n<ul>\n<li>总参数可达数千亿级，但<strong>每 token 激活</strong>仅约 <strong>数十亿到数百亿</strong>参数。</li>\n<li>强调<strong>中文、代码、数学推理</strong>等场景的实用性与性价比。</li>\n</ul>\n<hr>\n<h2 id=\"2-MoE-架构剖析\"><a href=\"#2-MoE-架构剖析\" class=\"headerlink\" title=\"2. MoE 架构剖析\"></a>2. MoE 架构剖析</h2><h3 id=\"2-1-稀疏激活（Sparse-Activation）\"><a href=\"#2-1-稀疏激活（Sparse-Activation）\" class=\"headerlink\" title=\"2.1 稀疏激活（Sparse Activation）\"></a>2.1 稀疏激活（Sparse Activation）</h3><p>在传统密集模型里，每个 token 都会计算同一套巨大的 FFN；MoE 将 FFN 替换为<strong>多个专家</strong>（experts）并由<strong>路由器</strong>（router/gating network）根据输入内容选择<strong>Top‑K</strong>个最相关专家参与计算。以常见配置为例：</p>\n<ul>\n<li>每层包含 <strong>256</strong> 个专家（含 1 个<strong>共享专家</strong>）；</li>\n<li>路由器为每个 token 选出 <strong>K=8</strong> 个专家，再<strong>加上共享专家</strong>一并参与（共 9 个）；</li>\n<li>整体模型总参数可达 <strong>≈6,710 亿</strong>，但<strong>每个 token 实际激活约 370 亿参数</strong>参与一次前向。</li>\n</ul>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>结果：在不牺牲容量的情形下，<strong>推理计算量</strong>近似等效于中型密集模型，<strong>吞吐与时延</strong>显著优化。</p></blockquote>\n<h3 id=\"2-2-专家路由与容量约束\"><a href=\"#2-2-专家路由与容量约束\" class=\"headerlink\" title=\"2.2 专家路由与容量约束\"></a>2.2 专家路由与容量约束</h3><p>路由器（可看作一个小型网络）对每个 token 计算到各专家的<strong>亲和分数</strong>，选 Top‑K 专家。为避免<strong>负载倾斜</strong>：</p>\n<ul>\n<li>为每位专家设置<strong>容量上限</strong>（一次最多接收多少 tokens），超过上限的 token 将被路由到次优专家；</li>\n<li>训练早期可引入<strong>随机抖动/温度</strong>，提升探索性，减少“路由塌陷”（总挑固定少数专家）。</li>\n</ul>\n<h3 id=\"2-3-无辅助损失的负载均衡\"><a href=\"#2-3-无辅助损失的负载均衡\" class=\"headerlink\" title=\"2.3 无辅助损失的负载均衡\"></a>2.3 无辅助损失的负载均衡</h3><p>传统 MoE 常在损失中加入均衡正则（例如 Switch 系列的 load‑balancing loss）。DeepSeek 实践了一种<strong>无显式辅助损失</strong>的均衡法：</p>\n<ul>\n<li>给专家引入<strong>可训练偏置</strong>（bias to routing score），让使用率偏低的专家获得更高被选概率；</li>\n<li>在不干扰主任务损失的前提下，<strong>自适应提升</strong>专家利用率的均衡性与稳定性；</li>\n<li>可辅以<strong>序列级的轻量约束</strong>，避免单个序列内出现极端不均衡。</li>\n</ul>\n<h3 id=\"2-4-共享专家与知识隔离\"><a href=\"#2-4-共享专家与知识隔离\" class=\"headerlink\" title=\"2.4 共享专家与知识隔离\"></a>2.4 共享专家与知识隔离</h3><p>设置一个<strong>共享专家（Shared Expert）</strong>在每个 token 上始终激活：</p>\n<ul>\n<li>将<strong>常识性/通用规律</strong>收敛在共享专家，减少在多个专家之间的知识重复；</li>\n<li>让其余专家专注于<strong>领域特化</strong>（如代码、数学、检索式问答等）；</li>\n<li>实践表明可增强<strong>总体稳定性</strong>与<strong>泛化</strong>。</li>\n</ul>\n<h3 id=\"2-5-专家特化与可扩展性\"><a href=\"#2-5-专家特化与可扩展性\" class=\"headerlink\" title=\"2.5 专家特化与可扩展性\"></a>2.5 专家特化与可扩展性</h3><p>MoE 的“模块化”带来两点收益：</p>\n<ul>\n<li><strong>特化</strong>：各专家围绕不同能力分工（代码/数学/常识/工具调用等），组合后能力多样；</li>\n<li><strong>可扩展</strong>：新增/替换专家几乎<strong>不改变</strong>单次推理计算量，便于<strong>按需扩容</strong>与<strong>领域定制</strong>。</li>\n</ul>\n<h3 id=\"2-6-结构对比与示意（Mermaid）\"><a href=\"#2-6-结构对比与示意（Mermaid）\" class=\"headerlink\" title=\"2.6 结构对比与示意（Mermaid）\"></a>2.6 结构对比与示意（Mermaid）</h3><h4 id=\"2-6-1-密集-FFN\"><a href=\"#2-6-1-密集-FFN\" class=\"headerlink\" title=\"2.6.1 密集 FFN\"></a>2.6.1 密集 FFN</h4><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">flowchart LR</span><br><span class=\"line\">  A[Token 表示] --&gt; B[自注意力层]</span><br><span class=\"line\">  B --&gt; C[密集 FFN (全参与)]</span><br><span class=\"line\">  C --&gt; D[输出]</span><br></pre></td></tr></table></figure>\n<h4 id=\"2-6-2-稀疏-MoE（Top‑K-Shared-Expert）\"><a href=\"#2-6-2-稀疏-MoE（Top‑K-Shared-Expert）\" class=\"headerlink\" title=\"2.6.2 稀疏 MoE（Top‑K + Shared Expert）\"></a>2.6.2 稀疏 MoE（Top‑K + Shared Expert）</h4><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">flowchart LR</span><br><span class=\"line\">  A[Token 表示] --&gt; B[自注意力层]</span><br><span class=\"line\">  B --&gt; R[路由器 Router]</span><br><span class=\"line\">  subgraph MoE Block</span><br><span class=\"line\">    R --&gt; E1[共享专家 Shared]</span><br><span class=\"line\">    R --&gt; E2[专家 1]</span><br><span class=\"line\">    R --&gt; E3[专家 2]</span><br><span class=\"line\">    R --&gt; E4[专家 ...]</span><br><span class=\"line\">    R --&gt; Ek[专家 K]</span><br><span class=\"line\">    E1 --&gt; M[专家输出聚合/加权求和]</span><br><span class=\"line\">    E2 --&gt; M</span><br><span class=\"line\">    E3 --&gt; M</span><br><span class=\"line\">    E4 --&gt; M</span><br><span class=\"line\">    Ek --&gt; M</span><br><span class=\"line\">  end</span><br><span class=\"line\">  M --&gt; D[输出]</span><br></pre></td></tr></table></figure>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>对比要点：密集 FFN 对所有 token <strong>全参与</strong>；MoE 通过 <strong>Router</strong> 仅激活<strong>少量专家+共享专家</strong>，再对结果聚合。</p></blockquote>\n<hr>\n<h2 id=\"3-训练策略与实现细节\"><a href=\"#3-训练策略与实现细节\" class=\"headerlink\" title=\"3. 训练策略与实现细节\"></a>3. 训练策略与实现细节</h2><h3 id=\"3-1-海量预训练与工程化\"><a href=\"#3-1-海量预训练与工程化\" class=\"headerlink\" title=\"3.1 海量预训练与工程化\"></a>3.1 海量预训练与工程化</h3><ul>\n<li><strong>数据规模</strong>：多语言为主，覆盖自然语言、代码、数学与科学文本；token 规模可达 <strong>≈14.8T</strong> 量级；</li>\n<li><strong>混合精度</strong>：在超大规模上实测 <strong>FP8</strong>（与 FP16/BF16 混用）可保持稳定收敛并显著提高吞吐；</li>\n<li><strong>分布式优化</strong>：深度流水并行、张量并行、专家并行，<strong>通信与计算重叠</strong>，显著降低“稀疏模型 = 通信密集”的开销；</li>\n<li><strong>算耗量级</strong>：完整预训练约 <strong>百万级 GPU·小时</strong>（如基于 H800/H100 集群），损失曲线稳定，无需大规模回滚。</li>\n</ul>\n<h3 id=\"3-2-多阶段微调：SFT-→-蒸馏-→-RL-对齐\"><a href=\"#3-2-多阶段微调：SFT-→-蒸馏-→-RL-对齐\" class=\"headerlink\" title=\"3.2 多阶段微调：SFT → 蒸馏 → RL 对齐\"></a>3.2 多阶段微调：SFT → 蒸馏 → RL 对齐</h3><ul>\n<li><strong>SFT（监督微调）</strong>：指令遵循、对话与工具使用示例，强化可控性与易用性；</li>\n<li><strong>从强推理模型蒸馏</strong>：将“长链思维/过程推理”示例蒸馏到通用模型，使其<strong>保留强推理</strong>同时<strong>输出更简洁</strong>；</li>\n<li><strong>RLHF / Group‑RPO 等</strong>：结合规则 + 模型奖励，优化<strong>有用性、真实性、安全性</strong>，抑制幻觉与不当输出。</li>\n</ul>\n<h3 id=\"3-3-训练目标改造：多-Token-并行预测（MTP）\"><a href=\"#3-3-训练目标改造：多-Token-并行预测（MTP）\" class=\"headerlink\" title=\"3.3 训练目标改造：多 Token 并行预测（MTP）\"></a>3.3 训练目标改造：多 Token 并行预测（MTP）</h3><ul>\n<li>在语言建模目标上引入 <strong>Multi‑Token Prediction</strong>：一次性预测后续多个 token；</li>\n<li>经验上可增强<strong>长依赖建模</strong>与<strong>推理连贯性</strong>；</li>\n<li>与<strong>推测解码（Speculative Decoding）</strong>天然兼容，小模型生成候选、大模型并行校验，加速推理。</li>\n</ul>\n<h3 id=\"3-4-长上下文\"><a href=\"#3-4-长上下文\" class=\"headerlink\" title=\"3.4 长上下文\"></a>3.4 长上下文</h3><ul>\n<li>支持<strong>超长上下文</strong>（典型可达 <strong>128K</strong> tokens 量级）；</li>\n<li>结合相对位置编码、多查询注意力/多头布局优化等手段，兼顾<strong>上下文长度</strong>与<strong>吞吐</strong>。</li>\n</ul>\n<hr>\n<h2 id=\"4-性能与取舍\"><a href=\"#4-性能与取舍\" class=\"headerlink\" title=\"4. 性能与取舍\"></a>4. 性能与取舍</h2><h3 id=\"4-1-能力画像\"><a href=\"#4-1-能力画像\" class=\"headerlink\" title=\"4.1 能力画像\"></a>4.1 能力画像</h3><ul>\n<li><strong>强项</strong>：代码生成、数学推理、中文任务（阅读理解、百科问答、写作）等；</li>\n<li><strong>短板</strong>：部分英文常识问答与事实性检索题可能略逊于同级别顶尖密集模型；</li>\n<li><strong>原因</strong>（可能）：路由导致知识分散、语料侧重差异、对齐偏好不同等。</li>\n</ul>\n<h3 id=\"4-2-推理速度与成本\"><a href=\"#4-2-推理速度与成本\" class=\"headerlink\" title=\"4.2 推理速度与成本\"></a>4.2 推理速度与成本</h3><ul>\n<li><strong>每 token 仅激活少量专家</strong>（如 8 个 + 共享专家），<strong>有效参数量 ≪ 总参数量</strong>；</li>\n<li>在相同硬件下，吞吐与时延<strong>优于</strong>同规模密集模型，<strong>单位成本显著降低</strong>；</li>\n<li>适合<strong>大规模在线服务</strong>与<strong>本地化部署</strong>的场景，具备极高<strong>性价比</strong>。</li>\n</ul>\n<h3 id=\"4-3-训练效率\"><a href=\"#4-3-训练效率\" class=\"headerlink\" title=\"4.3 训练效率\"></a>4.3 训练效率</h3><ul>\n<li>相比“同等效果”的密集模型，MoE 训练<strong>算耗可降到量级更低</strong>（经验上可达<strong>约 1/10</strong>）；</li>\n<li>关键在于：通信优化 + 容量约束 + 负载均衡 + 稳定路由，使稀疏训练可控、可复现。</li>\n</ul>\n<hr>\n<h2 id=\"5-研发陷阱与工程要点（实践向）\"><a href=\"#5-研发陷阱与工程要点（实践向）\" class=\"headerlink\" title=\"5. 研发陷阱与工程要点（实践向）\"></a>5. 研发陷阱与工程要点（实践向）</h2><ul>\n<li><strong>路由塌陷</strong>：早期加入温度与随机性；使用容量上限与可训练偏置提升均衡性。  </li>\n<li><strong>梯度不稳/爆炸</strong>：梯度裁剪、优化器学习率热身、损失缩放（AMP）、稳定化激活（如 SwiGLU）。  </li>\n<li><strong>通信瓶颈</strong>：专家并行 + token 重排/聚合；流水并行分层；尽量<strong>计算‑通信重叠</strong>。  </li>\n<li><strong>内存开销</strong>：总权重需要常驻（参数并行/ZeRO），推理可结合<strong>专家裁剪/蒸馏</strong>降低显存；离线量化（INT8/INT4）在部署侧非常有效。  </li>\n<li><strong>评测与对齐</strong>：区分<strong>推理能力</strong>与<strong>事实性</strong>；中文/代码/数学单列评估，英文常识补强检索/RAG。</li>\n</ul>\n<hr>\n<h2 id=\"6-总结与展望\"><a href=\"#6-总结与展望\" class=\"headerlink\" title=\"6. 总结与展望\"></a>6. 总结与展望</h2><ul>\n<li><strong>总结</strong>：DeepSeek 通过 <strong>MoE 稀疏专家</strong>实现了“<strong>大容量、低计算</strong>”的良性组合；配合 <strong>FP8 + 并行与通信优化 + 蒸馏 + RL 对齐 + MTP</strong>，在<strong>中文、代码、数学</strong>等方向形成<strong>高性价比</strong>优势。  </li>\n<li><strong>趋势</strong>：<ul>\n<li><strong>思考模式切换</strong>（快答 vs. 逐步推理）以提升复杂任务效率；</li>\n<li><strong>Agent 化与工具使用</strong>增强；</li>\n<li><strong>动态专家</strong>与<strong>更智能的路由</strong>、多模态 MoE、跨任务/跨域专家共享。  </li>\n</ul>\n</li>\n<li><strong>对开发者的建议</strong>：<br>1) 小规模上先验证 <strong>Top‑K、容量、负载均衡</strong>超参对稳定性的影响；<br>2) 将<strong>领域数据</strong>做成“专家‑友好”的微调/蒸馏集，促成<strong>专家特化</strong>；<br>3) 部署侧优先做<strong>量化 + KV Cache 优化 + 推测解码</strong>，在保证质量的前提下最大化吞吐。</li>\n</ul>\n<hr>\n<h3 id=\"附录：一页速览（Cheatsheet）\"><a href=\"#附录：一页速览（Cheatsheet）\" class=\"headerlink\" title=\"附录：一页速览（Cheatsheet）\"></a>附录：一页速览（Cheatsheet）</h3><ul>\n<li><strong>MoE = 大容量 + 稀疏计算</strong>；Top‑K + Shared Expert 是高性价比默认配置。  </li>\n<li><strong>稳定训练关键</strong>：容量约束、可训练偏置均衡、通信重叠、FP8、梯度裁剪。  </li>\n<li><strong>能力结构</strong>：中文/代码/数学强，英文常识可用 RAG/检索补齐。  </li>\n<li><strong>部署优先级</strong>：量化 → KV 缓存 → 推测解码 → 并行流水化。</li>\n</ul>\n","feature":true,"text":"面向具有一定深度学习基础的读者，本文系统解析 DeepSeek 系列模型的原理与设计特点，重点关注其 Mixture-of-Experts (MoE) 稀疏架构...","permalink":"/post/deepseek_moe","photos":[],"count_time":{"symbolsCount":"3.7k","symbolsTime":"3 mins."},"categories":[{"name":"人工智能","slug":"人工智能","count":3,"path":"api/categories/人工智能.json"}],"tags":[{"name":"deepseek","slug":"deepseek","count":1,"path":"api/tags/deepseek.json"},{"name":"MoE","slug":"MoE","count":1,"path":"api/tags/MoE.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#1-%E6%95%B4%E4%BD%93%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9A%E4%BD%8D\"><span class=\"toc-text\">1. 整体设计与定位</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#2-MoE-%E6%9E%B6%E6%9E%84%E5%89%96%E6%9E%90\"><span class=\"toc-text\">2. MoE 架构剖析</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#2-1-%E7%A8%80%E7%96%8F%E6%BF%80%E6%B4%BB%EF%BC%88Sparse-Activation%EF%BC%89\"><span class=\"toc-text\">2.1 稀疏激活（Sparse Activation）</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#2-2-%E4%B8%93%E5%AE%B6%E8%B7%AF%E7%94%B1%E4%B8%8E%E5%AE%B9%E9%87%8F%E7%BA%A6%E6%9D%9F\"><span class=\"toc-text\">2.2 专家路由与容量约束</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#2-3-%E6%97%A0%E8%BE%85%E5%8A%A9%E6%8D%9F%E5%A4%B1%E7%9A%84%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1\"><span class=\"toc-text\">2.3 无辅助损失的负载均衡</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#2-4-%E5%85%B1%E4%BA%AB%E4%B8%93%E5%AE%B6%E4%B8%8E%E7%9F%A5%E8%AF%86%E9%9A%94%E7%A6%BB\"><span class=\"toc-text\">2.4 共享专家与知识隔离</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#2-5-%E4%B8%93%E5%AE%B6%E7%89%B9%E5%8C%96%E4%B8%8E%E5%8F%AF%E6%89%A9%E5%B1%95%E6%80%A7\"><span class=\"toc-text\">2.5 专家特化与可扩展性</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#2-6-%E7%BB%93%E6%9E%84%E5%AF%B9%E6%AF%94%E4%B8%8E%E7%A4%BA%E6%84%8F%EF%BC%88Mermaid%EF%BC%89\"><span class=\"toc-text\">2.6 结构对比与示意（Mermaid）</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#2-6-1-%E5%AF%86%E9%9B%86-FFN\"><span class=\"toc-text\">2.6.1 密集 FFN</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#2-6-2-%E7%A8%80%E7%96%8F-MoE%EF%BC%88Top%E2%80%91K-Shared-Expert%EF%BC%89\"><span class=\"toc-text\">2.6.2 稀疏 MoE（Top‑K + Shared Expert）</span></a></li></ol></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#3-%E8%AE%AD%E7%BB%83%E7%AD%96%E7%95%A5%E4%B8%8E%E5%AE%9E%E7%8E%B0%E7%BB%86%E8%8A%82\"><span class=\"toc-text\">3. 训练策略与实现细节</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#3-1-%E6%B5%B7%E9%87%8F%E9%A2%84%E8%AE%AD%E7%BB%83%E4%B8%8E%E5%B7%A5%E7%A8%8B%E5%8C%96\"><span class=\"toc-text\">3.1 海量预训练与工程化</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#3-2-%E5%A4%9A%E9%98%B6%E6%AE%B5%E5%BE%AE%E8%B0%83%EF%BC%9ASFT-%E2%86%92-%E8%92%B8%E9%A6%8F-%E2%86%92-RL-%E5%AF%B9%E9%BD%90\"><span class=\"toc-text\">3.2 多阶段微调：SFT → 蒸馏 → RL 对齐</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#3-3-%E8%AE%AD%E7%BB%83%E7%9B%AE%E6%A0%87%E6%94%B9%E9%80%A0%EF%BC%9A%E5%A4%9A-Token-%E5%B9%B6%E8%A1%8C%E9%A2%84%E6%B5%8B%EF%BC%88MTP%EF%BC%89\"><span class=\"toc-text\">3.3 训练目标改造：多 Token 并行预测（MTP）</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#3-4-%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87\"><span class=\"toc-text\">3.4 长上下文</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#4-%E6%80%A7%E8%83%BD%E4%B8%8E%E5%8F%96%E8%88%8D\"><span class=\"toc-text\">4. 性能与取舍</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#4-1-%E8%83%BD%E5%8A%9B%E7%94%BB%E5%83%8F\"><span class=\"toc-text\">4.1 能力画像</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#4-2-%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6%E4%B8%8E%E6%88%90%E6%9C%AC\"><span class=\"toc-text\">4.2 推理速度与成本</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#4-3-%E8%AE%AD%E7%BB%83%E6%95%88%E7%8E%87\"><span class=\"toc-text\">4.3 训练效率</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#5-%E7%A0%94%E5%8F%91%E9%99%B7%E9%98%B1%E4%B8%8E%E5%B7%A5%E7%A8%8B%E8%A6%81%E7%82%B9%EF%BC%88%E5%AE%9E%E8%B7%B5%E5%90%91%EF%BC%89\"><span class=\"toc-text\">5. 研发陷阱与工程要点（实践向）</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#6-%E6%80%BB%E7%BB%93%E4%B8%8E%E5%B1%95%E6%9C%9B\"><span class=\"toc-text\">6. 总结与展望</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E9%99%84%E5%BD%95%EF%BC%9A%E4%B8%80%E9%A1%B5%E9%80%9F%E8%A7%88%EF%BC%88Cheatsheet%EF%BC%89\"><span class=\"toc-text\">附录：一页速览（Cheatsheet）</span></a></li></ol></li></ol>","author":{"name":"Rockway","slug":"blog-author","avatar":"https://lingmafuture.github.io/images/3.jpg","link":"https://lingmafuture.github.io","description":"一个充满情怀的AI技术探索者，欢迎交流人工智能、量化交易、创业灵感。","socials":{"github":"https://github.com/lingmafuture","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"mapped":true,"hidden":false,"prev_post":{},"next_post":{"title":"多模态行人 ReID 全量微调的过拟合问题与解决方案","uid":"32fbe71fdaa98cf4cb13d70c56ad0143","slug":"multimodal_reid","date":"2025-08-19T16:00:00.000Z","updated":"2025-08-19T13:21:36.221Z","comments":true,"path":"api/articles/multimodal_reid.json","keywords":"blog, technology, programming","cover":"images/multimodal_reid.png","text":"多模态行人重识别（ReID）旨在利用多种数据模态（如可见光图像、红外图像、素描、彩色手绘图、文本描述等）来匹配行人身份。在最新的 ORBench 数据集中，每个...","permalink":"/post/multimodal_reid","photos":[],"count_time":{"symbolsCount":"10k","symbolsTime":"9 mins."},"categories":[{"name":"人工智能","slug":"人工智能","count":3,"path":"api/categories/人工智能.json"}],"tags":[{"name":"多模态","slug":"多模态","count":1,"path":"api/tags/多模态.json"},{"name":"CV","slug":"CV","count":1,"path":"api/tags/CV.json"}],"author":{"name":"Rockway","slug":"blog-author","avatar":"https://lingmafuture.github.io/images/3.jpg","link":"https://lingmafuture.github.io","description":"一个充满情怀的AI技术探索者，欢迎交流人工智能、量化交易、创业灵感。","socials":{"github":"https://github.com/lingmafuture","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"feature":true}}